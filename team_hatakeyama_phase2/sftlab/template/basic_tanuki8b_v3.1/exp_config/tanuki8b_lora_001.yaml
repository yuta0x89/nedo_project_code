data:
  - name: Aratako/Synthetic-JP-Coding-Dataset-Magpie-Nemotron-4-10k
    preprocess:
      template: Alpaca_OpenAI_messages
      keys:
        messages: messages
    split:
      train: train
  - name: team-hatakeyama-phase2/OpenBookQA-Japanese
    preprocess:
      template: Alpaca
      keys:
        instruction: question_stem
        output: response
    split:
      train: train
      eval: validation

model:
  name: /storage5/llm/models/hf/step62160_fin

exp_params:
  num_train_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 64
  save_strategy: steps
  save_steps: 1000
  logging_steps: 1
  learning_rate: 1e-4
  warmup_ratio: 0.1
  lr_scheduler_type: cosine
  dtype: bf16
  use_fast: true
  instruction_template: \n\n### 指示:\n
  response_template: \n\n### 応答:\n
  gradient_checkpointing: true
  max_seq_length: 4096
  use_peft: true
  peft_target_model: llama-all
  use_flash_attention_2: true
  peft_lora_r: 128
  peft_lora_alpha: 256
  peft_lora_dropout: 0.05
  neftune_noise_alpha: null

  do_eval: true
  eval_strategy: steps
  eval_steps: 50