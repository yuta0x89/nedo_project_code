data:
  - name: team-hatakeyama-phase2/sft_baseline_100k
    colnames:
      instruction: instruction
      input: input
      output: output
    split:
      train: train
      eval: eval
  - name: team-hatakeyama-phase2/OpenBookQA-Japanese
    colnames:
      instruction: question_stem
      output: response
    split:
      train: train
  
model:
  name: /storage5/llm/models/hf/step62160_fin

exp_params:
  num_train_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  save_strategy: steps
  save_steps: 1000
  logging_steps: 1
  learning_rate: 5e-5
  warmup_ratio: 0.1
  lr_scheduler_type: cosine
  dtype: bf16
  use_fast: true
  instruction_template: \n\n### 指示:\n
  response_template: \n\n### 応答:\n
  gradient_checkpointing: true
  max_seq_length: 4096
  use_peft: false
  peft_target_model: llama-all
  use_flash_attention_2: true
  peft_lora_r: 128
  peft_lora_alpha: 256
  peft_lora_dropout: 0.05
  neftune_noise_alpha: null

  do_eval: true
  eval_strategy: steps
  eval_steps: 50