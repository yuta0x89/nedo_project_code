{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "国会図書館データを取得するプログラムです。  \n",
        "https://dl.ndl.go.jp\n",
        "\n",
        "このプログラムでは以下のURLよりダウンロードしたエクセルデータを利用します。  \n",
        "https://www.ndl.go.jp/jp/dlib/standards/opendataset/index.html\n",
        "\n",
        "今回利用したエクセルファイルは全部で4つです\n",
        "- 図書 2ファイル\n",
        "- 古典籍 1ファイル\n",
        "- 雑誌 1ファイル\n",
        "\n",
        "動作環境\n",
        "- Google Colab Pro\n",
        "- ハイメモリ\n",
        "- GPU 不要"
      ],
      "metadata": {
        "id": "gDqNMUBYjGP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "cLbEUt9JQXBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "lLC0FI5wRA39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CcxdDawJj5B"
      },
      "outputs": [],
      "source": [
        "!pip install datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X4e5EQIIjbg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from huggingface_hub import HfApi, HfFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKBUm3fdIn3X"
      },
      "outputs": [],
      "source": [
        "# Hugging Faceのトークンを環境変数に設定\n",
        "os.environ['HF_TOKEN'] = 'YOUR API KEY'\n",
        "\n",
        "# Hugging Faceのトークンを保存\n",
        "HfFolder.save_token(os.getenv('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjIiSRu57fAr"
      },
      "outputs": [],
      "source": [
        "class ExcelDataProcessor:\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.data = None\n",
        "\n",
        "    def load_data(self):\n",
        "        if self.data is None:\n",
        "            print(\"Loading Excel file...\")\n",
        "            self.data = pd.read_excel(self.file_path)\n",
        "            self.data['PID'] = self.data['永続的識別子'].str.extract(r'/(\\d+)$')\n",
        "            self.data = self.data[self.data['権利区分'] == '保護期間満了'].reset_index(drop=True)\n",
        "            print(\"Excel file loaded and filtered.\")\n",
        "        return self.data\n",
        "\n",
        "    def get_data_length(self):\n",
        "        if self.data is None:\n",
        "            self.load_data()\n",
        "        return len(self.data)\n",
        "\n",
        "    def process_data(self, _destination_folder, _download_folder, start_index=0, end_index=None):\n",
        "        filtered_data = self.load_data()\n",
        "\n",
        "        if end_index is None or end_index > len(filtered_data):\n",
        "            end_index = len(filtered_data)\n",
        "        filtered_data = filtered_data.iloc[start_index:end_index].copy()\n",
        "\n",
        "        text_contents = []\n",
        "\n",
        "        print(\"Starting download and reading text files...\")\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            futures = []\n",
        "            for pid in filtered_data['PID']:\n",
        "                future = executor.submit(self.download_and_extract, pid, _destination_folder, _download_folder)\n",
        "                futures.append(future)\n",
        "\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing PIDs\"):\n",
        "                try:\n",
        "                    content = future.result()\n",
        "                    text_contents.append(content)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error occurred: {e}\")\n",
        "                    text_contents.append(None)\n",
        "\n",
        "        print(\"Inserting contents...\")\n",
        "\n",
        "        filtered_data.insert(2, \"本文\", text_contents)\n",
        "        print(filtered_data.head())\n",
        "        return filtered_data\n",
        "\n",
        "    @staticmethod\n",
        "    def download_and_extract(pid, destination_folder, _download_folder):\n",
        "      try:\n",
        "        # 対象フォルダ内にすでに{pid}.txtがある場合はテキストを読み込む\n",
        "        if os.path.exists(os.path.join(destination_folder, f\"{pid}.txt\")):\n",
        "          with open(os.path.join(destination_folder, f\"{pid}.txt\"), 'r') as file:\n",
        "              content = file.read()\n",
        "          return content\n",
        "\n",
        "        # ZIPファイルのダウンロード\n",
        "        url = f\"https://lab.ndl.go.jp/dl/api/book/fulltext/{pid}\"\n",
        "        zip_file_path = os.path.join(_download_folder, f\"{pid}.zip\")\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # HTTPエラーが発生した場合の処理\n",
        "\n",
        "        with open(zip_file_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "\n",
        "        # ZIPファイルの解凍\n",
        "        extract_folder = os.path.join(_download_folder, pid)\n",
        "        os.makedirs(extract_folder, exist_ok=True)\n",
        "        with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(extract_folder)\n",
        "\n",
        "        # テキストファイルの移動および結合\n",
        "        source_file = os.path.join(extract_folder, f\"{pid}.txt\")\n",
        "        if os.path.exists(extract_folder) and not os.path.exists(source_file):\n",
        "            text_files = sorted([fname for fname in os.listdir(extract_folder) if fname.endswith('.txt')])\n",
        "            combined_content = \"\"\n",
        "            for text_file in text_files:\n",
        "                with open(os.path.join(extract_folder, text_file), 'r') as file:\n",
        "                    combined_content += file.read() + \"\\n\"\n",
        "\n",
        "            destination_file = os.path.join(destination_folder, f\"{pid}.txt\")\n",
        "            with open(destination_file, 'w') as file:\n",
        "                file.write(combined_content)\n",
        "        else:\n",
        "            shutil.move(source_file, os.path.join(destination_folder, f\"{pid}.txt\"))\n",
        "\n",
        "        # 作業用のZIPファイルと解凍されたフォルダを削除\n",
        "        os.remove(zip_file_path)\n",
        "        shutil.rmtree(extract_folder)\n",
        "\n",
        "        with open(os.path.join(destination_folder, f\"{pid}.txt\"), 'r') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "\n",
        "      except zipfile.BadZipFile:\n",
        "          print(f\"Failed to process PID {pid}: Bad ZIP file\")\n",
        "          return None\n",
        "      except OSError as e:\n",
        "          print(f\"OS error for PID {pid}: {e}\")\n",
        "          return None\n",
        "      except Exception as e:\n",
        "          print(f\"Failed to process PID {pid}: {e}\")\n",
        "          return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGGbqpbj95go"
      },
      "outputs": [],
      "source": [
        "# エクセルファイルのパス\n",
        "working_folder = \"/content/drive/MyDrive/Colab Notebooks/GENIAC/ndlj\"\n",
        "download_folder = \"/content\"\n",
        "\n",
        "# リスト形式でエクセルファイルを渡せるようにしたが、処理時間が長くなるので1ファイルずつ実行した方が良い\n",
        "# tosho_2 の batch_size = 8000\n",
        "\n",
        "file_names = [\"tosho_1\"]\n",
        "batch_size = 10000\n",
        "\n",
        "for file_name in file_names:\n",
        "  excel_file_path = working_folder + \"/\" + file_name + \".xlsx\"\n",
        "  hugging_face_repogitory_name = \"ndlj_\" + file_name\n",
        "\n",
        "  # ダウンロードとファイル操作を行うフォルダ\n",
        "  destination_folder = working_folder+ \"/text_files/\" + file_name\n",
        "\n",
        "  processor = ExcelDataProcessor(excel_file_path)\n",
        "\n",
        "  # データの長さを取得\n",
        "  data_length = processor.get_data_length()\n",
        "  print(f\"Total number of rows in the processed data: {data_length}\")\n",
        "\n",
        "  # 全バッチ数の計算\n",
        "  total_batches = (data_length + batch_size - 1) // batch_size\n",
        "\n",
        "  for batch_no in range(1, total_batches + 1):\n",
        "      start_number = batch_size * (batch_no - 1)\n",
        "      end_number = min(batch_size * batch_no, data_length)  # 最後のバッチがデータ長を超えないようにする\n",
        "\n",
        "      padded_batch_no = str(batch_no).zfill(2)\n",
        "      padded_start = str(start_number).zfill(6)\n",
        "      padded_end = str(end_number - 1).zfill(6)\n",
        "      hf_directory_name = f\"batch_no{padded_batch_no}_{padded_start}_to_{padded_end}\"\n",
        "\n",
        "      # データの処理\n",
        "      df = processor.process_data(destination_folder, download_folder, start_index=start_number, end_index=end_number)\n",
        "\n",
        "      # CSVファイルとして保存\n",
        "      csv_file_path = f\"{working_folder}/csv_files/{file_name}_{hf_directory_name}.csv\"\n",
        "      df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "      # pandasデータフレームをHugging Faceデータセットに変換\n",
        "      dataset = Dataset.from_pandas(df)\n",
        "\n",
        "      # データフレームのメモリを解放\n",
        "      del df\n",
        "      gc.collect()\n",
        "\n",
        "      # データセットのアップロード\n",
        "      dataset.push_to_hub(\n",
        "            hugging_face_repogitory_name,\n",
        "            private=True,\n",
        "            data_dir=hf_directory_name,\n",
        "            # create_pr=True,\n",
        "            # commit_description=f\"upload data, {hf_directory_name}\",\n",
        "            )\n",
        "      print(f\"Uploaded batch {batch_no} for file {file_name}\")\n",
        "\n",
        "      # データセットのメモリを解放\n",
        "      del dataset\n",
        "      gc.collect()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}