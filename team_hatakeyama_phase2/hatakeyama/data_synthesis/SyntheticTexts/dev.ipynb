{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.system(\"mkdir -p out_data\")\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")\n",
    "out_path = f\"out_data/model_{current_time_no_symbols}.jsonl\"\n",
    "\n",
    "\n",
    "from vllm import LLM\n",
    "import string\n",
    "from vllm import SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"microsoft/Phi-3-medium-128k-instruct\"\n",
    "#model_name=\"OrionStarAI/Orion-14B-Chat\"\n",
    "llm = LLM(model=model_name,trust_remote_code=True,\n",
    "          max_model_len=20000\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ds=load_dataset(\"kanhatakeyama/ChatbotArenaJaMixtral8x22b\", split=\"train\")\n",
    "#ds=load_dataset(\"wikipedia\", \"20220301.en\",streaming=True,split=\"train\")\n",
    "#ds=load_dataset(\"hpprc/jawiki-wiktionary\", split=\"train\")\n",
    "\n",
    "streaming=True\n",
    "ds_list=[\n",
    "load_dataset(\"HuggingFaceTB/cosmopedia\",\"auto_math_text\",streaming=streaming,split=\"train\"),\n",
    "load_dataset(\"HuggingFaceTB/cosmopedia\",\"khanacademy\",streaming=streaming,split=\"train\"),\n",
    "load_dataset(\"HuggingFaceTB/cosmopedia\",\"openstax\",streaming=streaming,split=\"train\"),\n",
    "load_dataset(\"HuggingFaceTB/cosmopedia\",\"stanford\",streaming=streaming,split=\"train\"),\n",
    "load_dataset(\"HuggingFaceTB/cosmopedia\",\"wikihow\",streaming=streaming,split=\"train\"),\n",
    "]\n",
    "ds=concatenate_datasets(ds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=concatenate_datasets([ds,ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds.shuffle()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "inst_dict={\n",
    "\n",
    "\"textbook\":\"\"\"次のデータをもとに､論理的かつ教科書調の丁寧な日本語の文章を作成しなさい｡\n",
    "-事実を正確に守り､推測出来ない事項については記述しないこと｡\n",
    "-元の文章の流用は避け､表現や段落分け､文体などを必ず変更すること｡\n",
    "-必ず日本語で出力すること\n",
    "\n",
    "#データ\n",
    "\"\"\",\n",
    "\n",
    "\"conversation\":\"\"\"次のデータをもとに､論理的な日本語の会話文を作成しなさい｡\n",
    "-事実を正確に守り､推測出来ない事項については記述しないこと｡\n",
    "-元の文章の流用は避け､表現や段落分け､文体などを必ず変更すること｡\n",
    "-必ず日本語で出力すること\n",
    "\n",
    "#データ\n",
    "\"\"\",\n",
    "\"logical\":\"\"\"次のデータをもとに､論理的な文章を作成しなさい｡\n",
    "-事実を正確に守り､推測出来ない事項については記述しないこと｡\n",
    "-元の文章の流用は避け､表現や段落分け､文体などを必ず変更すること｡\n",
    "-必ず日本語で出力すること\n",
    "\n",
    "#データ\n",
    "\"\"\"\n",
    "}\n",
    "def extract_random_part(text):\n",
    "    text_length = len(text)\n",
    "    extract_length = min(text_length, random.randint(400, 2000))\n",
    "    start_index = random.randint(0, text_length - extract_length)\n",
    "    return text[start_index:start_index + extract_length]\n",
    "\n",
    "mode_list=list(inst_dict.keys())\n",
    "n_records=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "loader=ds\n",
    "def prepare_records(loader,n_records=10):\n",
    "\n",
    "    records=[]\n",
    "    cnt=0\n",
    "    for record in loader:\n",
    "        mode=random.choice(mode_list)\n",
    "        inst=inst_dict[mode]\n",
    "        text=record[\"text\"]\n",
    "        text=extract_random_part(text)\n",
    "        text=f\"\"\"<|user|>\n",
    "    {inst}{text}<|end|>\n",
    "    <|assistant|>\"\"\"\n",
    "        records.append(\n",
    "            {\"original_text\":text,\n",
    "            \"mode\":mode,\n",
    "            \"url\":record[\"url\"]\n",
    "            }\n",
    "                    )\n",
    "        cnt+=1\n",
    "        if cnt>n_records:\n",
    "            break\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    records=prepare_records(loader,n_records)\n",
    "    prompts=[record[\"original_text\"] for record in records]\n",
    "    outputs = llm.generate(\n",
    "            prompts,\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.1,\n",
    "                max_tokens=1024,\n",
    "                repetition_penalty=1.2,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for record,output in zip(records,outputs):\n",
    "        record[\"output_text\"]=(output.outputs[0].text).strip()\n",
    "        record.pop(\"original_text\")\n",
    "        with open(out_path, \"a\") as f:\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
