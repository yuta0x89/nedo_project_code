wandb:
  log: True
  entity: "vision-language-leaderboard"
  project: "heron-leaderboard"
  run_name: "models/gemini-1.0-pro-vision-latest"
  launch: false

# basic information
testmode: false
torch_dtype: "fp16" # {fp16, bf16, fp32}
# if you don't use api, please set "api" as "false"
# if you use api, please select from "openai", "anthoropic", "google", "cohere", "mistral", "amazon_bedrock"
api: 'gemini'
model_artifact: null
processor_artifact: null
tokenizer_artifact: null

model:
  _target_: transformers.AutoModelForVision2Seq.from_pretrained
  pretrained_model_name_or_path: "models/gemini-1.0-pro-vision-latest"
  torch_dtype: "float16"
  ignore_mismatched_sizes: true

processor:
  _target_: transformers.AutoImageProcessor.from_pretrained
  pretrained_model_name_or_path: null
  
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: null

datasets:
  llava_bench_in_the_wild_artifact_path: 'vision-language-leaderboard/heron-leaderboard/llava-bench-in-the-wild:v0'
  llava_bench_in_the_wild_reference_path: 'vision-language-leaderboard/heron-leaderboard/llava-bench-in-the-wild-reference:v0'
  japanese_heron_bench_artifact_path: 'vision-language-leaderboard/heron-leaderboard/japanese-heron-bench:v0'
  japanese_heron_bench_reference_path: 'vision-language-leaderboard/heron-leaderboard/heron-bench-reference:v0'

generation:
  args:
    max_length: 256
    do_sample: false
    temperature: 0.0
    top_p: 0.0
    no_repeat_ngram_size: 2