{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2a4a36-bc31-41a8-8ce7-757e70cc869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 21:56:40.171953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-26 21:56:40.184002: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-26 21:56:40.187628: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-26 21:56:40.197036: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-26 21:56:40.890305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 21:56:42,402] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/own/conda/envs/Dev/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "from datasets import features, load_dataset, Dataset\n",
    "import torch\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from llavajp.model.llava_llama import LlavaLlamaForCausalLM\n",
    "from llavajp.model.llava_gpt2 import LlavaGpt2ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9578c733-1083-4697-ae11-c189de3c278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "\n",
    "from transformers.feature_extraction_utils import BatchFeature\n",
    "from transformers.image_utils import ImageInput\n",
    "from transformers.processing_utils import ProcessorMixin\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n",
    "from transformers.utils import TensorType\n",
    "\n",
    "\n",
    "class LlavaProcessor(ProcessorMixin):\n",
    "    r\"\"\"\n",
    "    Constructs a Llava processor which wraps a Llava image processor and a Llava tokenizer into a single processor.\n",
    "\n",
    "    [`LlavaProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`LlamaTokenizerFast`]. See the\n",
    "    [`~LlavaProcessor.__call__`] and [`~LlavaProcessor.decode`] for more information.\n",
    "\n",
    "    Args:\n",
    "        image_processor ([`CLIPImageProcessor`], *optional*):\n",
    "            The image processor is a required input.\n",
    "        tokenizer ([`LlamaTokenizerFast`], *optional*):\n",
    "            The tokenizer is a required input.\n",
    "        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n",
    "            in a chat into a tokenizable string.\n",
    "    \"\"\"\n",
    "\n",
    "    attributes = [\"image_processor\", \"tokenizer\"]\n",
    "    valid_kwargs = [\"chat_template\"]\n",
    "    image_processor_class = \"SiglipImageProcessor\"\n",
    "    tokenizer_class = \"AutoTokenizer\"\n",
    "\n",
    "    def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n",
    "        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n",
    "        images: ImageInput = None,\n",
    "        padding: Union[bool, str, PaddingStrategy] = False,\n",
    "        truncation: Union[bool, str, TruncationStrategy] = None,\n",
    "        max_length=None,\n",
    "        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n",
    "    ) -> BatchFeature:\n",
    "        \"\"\"\n",
    "        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n",
    "        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n",
    "        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n",
    "        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n",
    "        of the above two methods for more information.\n",
    "\n",
    "        Args:\n",
    "            text (`str`, `List[str]`, `List[List[str]]`):\n",
    "                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
    "                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
    "                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
    "            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n",
    "                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n",
    "                tensor. Both channels-first and channels-last formats are supported.\n",
    "            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
    "                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
    "                index) among:\n",
    "                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "                  sequence if provided).\n",
    "                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "                  acceptable input length for the model if that argument is not provided.\n",
    "                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
    "                  lengths).\n",
    "            max_length (`int`, *optional*):\n",
    "                Maximum length of the returned list and optionally padding length (see above).\n",
    "            truncation (`bool`, *optional*):\n",
    "                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n",
    "            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
    "                If set, will return tensors of a particular framework. Acceptable values are:\n",
    "\n",
    "                - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
    "                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
    "                - `'np'`: Return NumPy `np.ndarray` objects.\n",
    "                - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
    "\n",
    "        Returns:\n",
    "            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n",
    "\n",
    "            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n",
    "            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
    "              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n",
    "              `None`).\n",
    "            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n",
    "        \"\"\"\n",
    "        if images is not None:\n",
    "            #pixel_values = self.image_processor(images, return_tensors=return_tensors, size={\"height\": 768, \"width\": 768})[\"pixel_values\"]\n",
    "            images = images.convert(\"RGB\")\n",
    "            pixel_values = self.image_processor(images, return_tensors=return_tensors)[\"pixel_values\"]\n",
    "        else:\n",
    "            pixel_values = None\n",
    "        text_inputs = self.tokenizer(\n",
    "            text, return_tensors=return_tensors, padding=padding, truncation=truncation, max_length=max_length\n",
    "        )\n",
    "\n",
    "        return BatchFeature(data={**text_inputs, \"pixel_values\": pixel_values})\n",
    "\n",
    "    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n",
    "    def batch_decode(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n",
    "        refer to the docstring of this method for more information.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.batch_decode(*args, **kwargs)\n",
    "\n",
    "    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n",
    "    def decode(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n",
    "        the docstring of this method for more information.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.decode(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n",
    "    def model_input_names(self):\n",
    "        tokenizer_input_names = self.tokenizer.model_input_names\n",
    "        image_processor_input_names = self.image_processor.model_input_names\n",
    "        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbd909a-841f-424c-bf04-cb5c3646e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the model and processor\n",
    "    #model = LlavaLlamaForCausalLM.from_pretrained(\"team-hatakeyama-phase2/Tanuki-8B-vision-v4-checkpoint-26000\", device_map=\"cuda\", torch_dtype=torch.bfloat16, cache_dir=\"cache\", attn_implementation=\"eager\")\n",
    "    model = LlavaGpt2ForCausalLM.from_pretrained(\"hibikaze/finetune-llava-v1.5-japanese-gpt2-small_test-checkpoint-1200\", device_map=\"cuda\", torch_dtype=torch.bfloat16, cache_dir=\"cache\", attn_implementation=\"eager\")\n",
    "\n",
    "    #processor = LlavaProcessor.from_pretrained(\"team-hatakeyama-phase2/Tanuki-8B-vision-v4-checkpoint-26000\", cache_dir=\"cache\")\n",
    "    processor = LlavaProcessor.from_pretrained(\"hibikaze/finetune-llava-v1.5-japanese-gpt2-small_test-checkpoint-1200\", cache_dir=\"cache\")\n",
    "\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", cache_dir=\"cache\")\n",
    "    LLAVA_CHAT_TEMPLATE = \"\"\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}これは好奇心旺盛なユーザーと人工知能システムのチャットです。システムはユーザーの質問に親切、詳細、丁寧に答える。 ユーザー: {% else %}システム: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>\\n{% endif %}{% endfor %}{% if message['role'] == 'user' %}{% else %}{{eos_token}}{% endif %}{% endfor %}{% if add_generation_prompt %}システム:  {% endif %}\"\"\"\n",
    "    processor.chat_template = LLAVA_CHAT_TEMPLATE\n",
    "    #tokenizer.chat_template = LLAVA_CHAT_TEMPLATE\n",
    "    #processor.tokenizer = tokenizer\n",
    "\n",
    "    print(\"===== processor =====\")\n",
    "    print(processor)\n",
    "    #print(\"===== tokenizer =====\")\n",
    "    #print(tokenizer)\n",
    "    print(\"=====================\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"hibikaze/wit-dpo-test\", split=\"train[:1%]\", cache_dir=\"cache\")\n",
    "\n",
    "    def format(example):\n",
    "        # Prepare the input for the chat template\n",
    "        prompt = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": example[\"question\"]}]}]\n",
    "        chosen = [{\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"chosen\"]}]}]\n",
    "        rejected = [{\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"rejected\"]}]}]\n",
    "        # Apply the chat template\n",
    "        prompt = processor.apply_chat_template(prompt, tokenize=False)\n",
    "        chosen = processor.apply_chat_template(chosen, tokenize=False)\n",
    "        rejected = processor.apply_chat_template(rejected, tokenize=False)\n",
    "        # Resize the image to ensure it fits within the maximum allowable\n",
    "        # size of the processor to prevent OOM errors.\n",
    "        #max_size = processor.image_processor.size[\"longest_edge\"] // 2\n",
    "        #example[\"image\"].thumbnail((max_size, max_size))\n",
    "        return {\"images\": example[\"image\"], \"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "        #return {\"images\": [example[\"image\"]], \"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "        #return {\"images\": [example[\"image\"].convert('RGB')], \"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "    # Apply the formatting function to the dataset\n",
    "    #dataset = dataset.map(format, remove_columns=dataset.column_names, num_proc=1)\n",
    "\n",
    "    # データセットをリスト形式に変換し、各サンプルに対してformat関数を適用\n",
    "    formatted_examples_list = [format(example) for example in tqdm(dataset)]\n",
    "\n",
    "    print(formatted_examples_list[0])\n",
    "    \n",
    "    # リストを再びデータセット形式に変換\n",
    "    formatted_dataset = Dataset.from_list(formatted_examples_list)\n",
    "\n",
    "    dataset = formatted_dataset\n",
    "\n",
    "    # Make sure that the images are decoded, it prevents from storing bytes.\n",
    "    # More info here https://github.com/huggingface/blog/pull/2148#discussion_r1667400478\n",
    "    #f = dataset.features\n",
    "    #f[\"images\"] = features.Sequence(features.Image(decode=True))\n",
    "    #dataset = dataset.cast(f)\n",
    "\n",
    "    # Train the model\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=\"output_llava/dpo/llava-jp\",\n",
    "        #output_dir=\"idefics2-8b-dpo\",\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=1,\n",
    "        dataset_num_proc=1,  # tokenization will use 32 processes\n",
    "        dataloader_num_workers=1,  # data loading will use 32 workers\n",
    "        logging_steps=2,\n",
    "    )\n",
    "\n",
    "    trainer = DPOTrainer(\n",
    "        model,\n",
    "        ref_model=None,  # not needed when using peft\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=processor,\n",
    "        #peft_config=LoraConfig(target_modules=\"all-linear\"),\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e3ecb8-b894-4d98-a051-82b36436aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava-jp. This is not supported for all configurations of models and can yield errors.\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.24.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.25.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.26.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.probe: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.in_proj_bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.attention.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.head.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== processor =====\n",
      "LlavaProcessor:\n",
      "- image_processor: SiglipImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 768,\n",
      "    \"width\": 768\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"SiglipImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 768,\n",
      "    \"width\": 768\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: T5TokenizerFast(name_or_path='hibikaze/finetune-llava-v1.5-japanese-gpt2-small_test-checkpoint-1200', vocab_size=32000, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '[SEP]', 'pad_token': '<unk>', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}\\u3053\\u308c\\u306f\\u597d\\u5947\\u5fc3\\u65fa\\u76db\\u306a\\u30e6\\u30fc\\u30b6\\u30fc\\u3068\\u4eba\\u5de5\\u77e5\\u80fd\\u30b7\\u30b9\\u30c6\\u30e0\\u306e\\u30c1\\u30e3\\u30c3\\u30c8\\u3067\\u3059\\u3002\\u30b7\\u30b9\\u30c6\\u30e0\\u306f\\u30e6\\u30fc\\u30b6\\u30fc\\u306e\\u8cea\\u554f\\u306b\\u89aa\\u5207\\u3001\\u8a73\\u7d30\\u3001\\u4e01\\u5be7\\u306b\\u7b54\\u3048\\u308b\\u3002 \\u30e6\\u30fc\\u30b6\\u30fc: {% else %}\\u30b7\\u30b9\\u30c6\\u30e0: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>\\n{% endif %}{% endfor %}{% if message['role'] == 'user' %}{% else %}{{eos_token}}{% endif %}{% endfor %}{% if add_generation_prompt %}\\u30b7\\u30b9\\u30c6\\u30e0:  {% endif %}\",\n",
      "  \"processor_class\": \"LlavaProcessor\"\n",
      "}\n",
      "\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 18/18 [00:00<00:00, 485.77it/s]\n",
      "/home/docker/LLaVA-JP/external_libs/trl/trl/trainer/dpo_trainer.py:394: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/docker/LLaVA-JP/external_libs/trl/trl/trainer/dpo_trainer.py:407: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/docker/LLaVA-JP/external_libs/trl/trl/trainer/dpo_trainer.py:442: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=300x225 at 0x7FDCCD4F0EE0>, 'prompt': 'これは好奇心旺盛なユーザーと人工知能システムのチャットです。システムはユーザーの質問に親切、詳細、丁寧に答える。 ユーザー: <image>\\n画像に含まれている物が何であるか', 'chosen': 'システム: 多気城は、常陸国筑波郡多気にあった日本の城。多気山城・城山城とも称する。現存する史料や遺物が少なく、謎の城とされてきた。</s>', 'rejected': 'システム: 多気城 (常陸国)</s>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <trl.trainer.dpo_trainer.DPOTrainer object at 0x7fdcd8ff0e80>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb56dfeacb1445d2a106b97382d8867d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhibikaze\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/docker/LLaVA-JP/wandb/run-20240726_215655-r7m0cxvx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hibikaze/huggingface/runs/r7m0cxvx' target=\"_blank\">output_llava/dpo/llava-jp</a></strong> to <a href='https://wandb.ai/hibikaze/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hibikaze/huggingface' target=\"_blank\">https://wandb.ai/hibikaze/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hibikaze/huggingface/runs/r7m0cxvx' target=\"_blank\">https://wandb.ai/hibikaze/huggingface/runs/r7m0cxvx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/tmp/own/conda/envs/Dev/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 1.0000, 1.0000, 1.0000]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.555600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.553100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.441600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.450200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.419000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.6941, -0.6941, -0.6941,  ..., -0.7490, -0.7490, -0.7490],\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.7569, -0.7569, -0.7569],\n",
      "          [-0.6784, -0.6784, -0.6784,  ..., -0.7569, -0.7569, -0.7569],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7490,  ..., -0.2627, -0.2157, -0.1922],\n",
      "          [ 0.6863,  0.6863,  0.7176,  ..., -0.2706, -0.2235, -0.2000],\n",
      "          [ 0.6314,  0.6392,  0.6706,  ..., -0.2784, -0.2314, -0.2078]],\n",
      "\n",
      "         [[-0.7569, -0.7569, -0.7569,  ..., -0.6549, -0.6549, -0.6549],\n",
      "          [-0.7490, -0.7490, -0.7490,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          [-0.7412, -0.7412, -0.7412,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7412,  ..., -0.2000, -0.1529, -0.1294],\n",
      "          [ 0.6863,  0.6863,  0.7176,  ..., -0.2078, -0.1608, -0.1373],\n",
      "          [ 0.6314,  0.6392,  0.6706,  ..., -0.2157, -0.1686, -0.1451]],\n",
      "\n",
      "         [[-0.7725, -0.7725, -0.7725,  ..., -0.6549, -0.6549, -0.6549],\n",
      "          [-0.7647, -0.7647, -0.7647,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          [-0.7569, -0.7569, -0.7569,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          ...,\n",
      "          [ 0.7412,  0.7412,  0.7569,  ..., -0.1686, -0.1294, -0.1059],\n",
      "          [ 0.6941,  0.7020,  0.7255,  ..., -0.1843, -0.1373, -0.1137],\n",
      "          [ 0.6471,  0.6549,  0.6863,  ..., -0.1922, -0.1451, -0.1216]]],\n",
      "\n",
      "\n",
      "        [[[-0.6941, -0.6941, -0.6941,  ..., -0.7490, -0.7490, -0.7490],\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.7569, -0.7569, -0.7569],\n",
      "          [-0.6784, -0.6784, -0.6784,  ..., -0.7569, -0.7569, -0.7569],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7490,  ..., -0.2627, -0.2157, -0.1922],\n",
      "          [ 0.6863,  0.6863,  0.7176,  ..., -0.2706, -0.2235, -0.2000],\n",
      "          [ 0.6314,  0.6392,  0.6706,  ..., -0.2784, -0.2314, -0.2078]],\n",
      "\n",
      "         [[-0.7569, -0.7569, -0.7569,  ..., -0.6549, -0.6549, -0.6549],\n",
      "          [-0.7490, -0.7490, -0.7490,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          [-0.7412, -0.7412, -0.7412,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7412,  ..., -0.2000, -0.1529, -0.1294],\n",
      "          [ 0.6863,  0.6863,  0.7176,  ..., -0.2078, -0.1608, -0.1373],\n",
      "          [ 0.6314,  0.6392,  0.6706,  ..., -0.2157, -0.1686, -0.1451]],\n",
      "\n",
      "         [[-0.7725, -0.7725, -0.7725,  ..., -0.6549, -0.6549, -0.6549],\n",
      "          [-0.7647, -0.7647, -0.7647,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          [-0.7569, -0.7569, -0.7569,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          ...,\n",
      "          [ 0.7412,  0.7412,  0.7569,  ..., -0.1686, -0.1294, -0.1059],\n",
      "          [ 0.6941,  0.7020,  0.7255,  ..., -0.1843, -0.1373, -0.1137],\n",
      "          [ 0.6471,  0.6549,  0.6863,  ..., -0.1922, -0.1451, -0.1216]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.6941, -0.6941, -0.6941,  ..., -0.7490, -0.7490, -0.7490],\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.7569, -0.7569, -0.7569],\n",
      "          [-0.6784, -0.6784, -0.6784,  ..., -0.7569, -0.7569, -0.7569],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7490,  ..., -0.2627, -0.2157, -0.1922],\n",
      "          [ 0.6863,  0.6863,  0.7176,  ..., -0.2706, -0.2235, -0.2000],\n",
      "          [ 0.6314,  0.6392,  0.6706,  ..., -0.2784, -0.2314, -0.2078]],\n",
      "\n",
      "         [[-0.7569, -0.7569, -0.7569,  ..., -0.6549, -0.6549, -0.6549],\n",
      "          [-0.7490, -0.7490, -0.7490,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          [-0.7412, -0.7412, -0.7412,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7412,  ..., -0.2000, -0.1529, -0.1294],\n",
      "          [ 0.6863,  0.6863,  0.7176,  ..., -0.2078, -0.1608, -0.1373],\n",
      "          [ 0.6314,  0.6392,  0.6706,  ..., -0.2157, -0.1686, -0.1451]],\n",
      "\n",
      "         [[-0.7725, -0.7725, -0.7725,  ..., -0.6549, -0.6549, -0.6549],\n",
      "          [-0.7647, -0.7647, -0.7647,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          [-0.7569, -0.7569, -0.7569,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          ...,\n",
      "          [ 0.7412,  0.7412,  0.7569,  ..., -0.1686, -0.1294, -0.1059],\n",
      "          [ 0.6941,  0.7020,  0.7255,  ..., -0.1843, -0.1373, -0.1137],\n",
      "          [ 0.6471,  0.6549,  0.6863,  ..., -0.1922, -0.1451, -0.1216]]],\n",
      "\n",
      "\n",
      "        [[[-0.6941, -0.6941, -0.6941,  ..., -0.7490, -0.7490, -0.7490],\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.7569, -0.7569, -0.7569],\n",
      "          [-0.6784, -0.6784, -0.6784,  ..., -0.7569, -0.7569, -0.7569],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7490,  ..., -0.2627, -0.2157, -0.1922],\n",
      "          [ 0.6863,  0.6863,  0.7176,  ..., -0.2706, -0.2235, -0.2000],\n",
      "          [ 0.6314,  0.6392,  0.6706,  ..., -0.2784, -0.2314, -0.2078]],\n",
      "\n",
      "         [[-0.7569, -0.7569, -0.7569,  ..., -0.6549, -0.6549, -0.6549],\n",
      "          [-0.7490, -0.7490, -0.7490,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          [-0.7412, -0.7412, -0.7412,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          ...,\n",
      "          [ 0.7255,  0.7255,  0.7412,  ..., -0.2000, -0.1529, -0.1294],\n",
      "          [ 0.6863,  0.6863,  0.7176,  ..., -0.2078, -0.1608, -0.1373],\n",
      "          [ 0.6314,  0.6392,  0.6706,  ..., -0.2157, -0.1686, -0.1451]],\n",
      "\n",
      "         [[-0.7725, -0.7725, -0.7725,  ..., -0.6549, -0.6549, -0.6549],\n",
      "          [-0.7647, -0.7647, -0.7647,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          [-0.7569, -0.7569, -0.7569,  ..., -0.6627, -0.6627, -0.6627],\n",
      "          ...,\n",
      "          [ 0.7412,  0.7412,  0.7569,  ..., -0.1686, -0.1294, -0.1059],\n",
      "          [ 0.6941,  0.7020,  0.7255,  ..., -0.1843, -0.1373, -0.1137],\n",
      "          [ 0.6471,  0.6549,  0.6863,  ..., -0.1922, -0.1451, -0.1216]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.6941, -0.7098, -0.7412,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [-0.6784, -0.6941, -0.7176,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [-0.6392, -0.6471, -0.6706,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.8118, -0.8118, -0.8118],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.8118, -0.8118, -0.8118],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.8118, -0.8118, -0.8118]],\n",
      "\n",
      "         [[-0.3882, -0.4039, -0.4275,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [-0.3725, -0.3882, -0.4118,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [-0.3333, -0.3490, -0.3647,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.8039, -0.8039, -0.8039],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.8039, -0.8039, -0.8039],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.8039, -0.8039, -0.8039]],\n",
      "\n",
      "         [[ 0.2314,  0.2157,  0.1922,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          [ 0.2471,  0.2314,  0.2078,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          [ 0.2784,  0.2706,  0.2549,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.7725, -0.7725, -0.7725]]],\n",
      "\n",
      "\n",
      "        [[[-0.6941, -0.7098, -0.7412,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [-0.6784, -0.6941, -0.7176,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [-0.6392, -0.6471, -0.6706,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.8118, -0.8118, -0.8118],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.8118, -0.8118, -0.8118],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.8118, -0.8118, -0.8118]],\n",
      "\n",
      "         [[-0.3882, -0.4039, -0.4275,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [-0.3725, -0.3882, -0.4118,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [-0.3333, -0.3490, -0.3647,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.8039, -0.8039, -0.8039],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.8039, -0.8039, -0.8039],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.8039, -0.8039, -0.8039]],\n",
      "\n",
      "         [[ 0.2314,  0.2157,  0.1922,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          [ 0.2471,  0.2314,  0.2078,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          [ 0.2784,  0.2706,  0.2549,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.7725, -0.7725, -0.7725]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.6941, -0.7098, -0.7412,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [-0.6784, -0.6941, -0.7176,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [-0.6392, -0.6471, -0.6706,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.8118, -0.8118, -0.8118],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.8118, -0.8118, -0.8118],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.8118, -0.8118, -0.8118]],\n",
      "\n",
      "         [[-0.3882, -0.4039, -0.4275,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [-0.3725, -0.3882, -0.4118,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [-0.3333, -0.3490, -0.3647,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.8039, -0.8039, -0.8039],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.8039, -0.8039, -0.8039],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.8039, -0.8039, -0.8039]],\n",
      "\n",
      "         [[ 0.2314,  0.2157,  0.1922,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          [ 0.2471,  0.2314,  0.2078,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          [ 0.2784,  0.2706,  0.2549,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.7725, -0.7725, -0.7725]]],\n",
      "\n",
      "\n",
      "        [[[-0.6941, -0.7098, -0.7412,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [-0.6784, -0.6941, -0.7176,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [-0.6392, -0.6471, -0.6706,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.8118, -0.8118, -0.8118],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.8118, -0.8118, -0.8118],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.8118, -0.8118, -0.8118]],\n",
      "\n",
      "         [[-0.3882, -0.4039, -0.4275,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [-0.3725, -0.3882, -0.4118,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [-0.3333, -0.3490, -0.3647,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.8039, -0.8039, -0.8039],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.8039, -0.8039, -0.8039],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.8039, -0.8039, -0.8039]],\n",
      "\n",
      "         [[ 0.2314,  0.2157,  0.1922,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          [ 0.2471,  0.2314,  0.2078,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          [ 0.2784,  0.2706,  0.2549,  ...,  0.4196,  0.4196,  0.4196],\n",
      "          ...,\n",
      "          [ 0.2078,  0.1765,  0.1216,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.0588, -0.0980, -0.1843,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.1451, -0.1922, -0.2863,  ..., -0.7725, -0.7725, -0.7725]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]],\n",
      "\n",
      "         [[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]],\n",
      "\n",
      "         [[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]]],\n",
      "\n",
      "\n",
      "        [[[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]],\n",
      "\n",
      "         [[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]],\n",
      "\n",
      "         [[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]],\n",
      "\n",
      "         [[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]],\n",
      "\n",
      "         [[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]]],\n",
      "\n",
      "\n",
      "        [[[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]],\n",
      "\n",
      "         [[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]],\n",
      "\n",
      "         [[0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          [0.9843, 0.9843, 0.9686,  ..., 0.6627, 0.6471, 0.6392],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 0.9765,  ..., 0.1765, 0.4510, 0.5451],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1765, 0.4510, 0.5373],\n",
      "          [1.0000, 1.0000, 0.9843,  ..., 0.1843, 0.4510, 0.5373]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.4353,  0.4275,  0.4039,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [ 0.4196,  0.4118,  0.3961,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [ 0.4039,  0.3961,  0.3804,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          ...,\n",
      "          [-0.7255, -0.7255, -0.7333,  ..., -0.9059, -0.9059, -0.9059],\n",
      "          [-0.7098, -0.7098, -0.7176,  ..., -0.9059, -0.9059, -0.9059],\n",
      "          [-0.7020, -0.7020, -0.7098,  ..., -0.9059, -0.9059, -0.9059]],\n",
      "\n",
      "         [[ 0.5294,  0.5216,  0.4980,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          [ 0.5137,  0.5059,  0.4902,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          [ 0.4980,  0.4902,  0.4745,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.6471, -0.6471, -0.6471,  ..., -0.8902, -0.8902, -0.8902]],\n",
      "\n",
      "         [[ 0.5137,  0.5059,  0.4824,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [ 0.4980,  0.4902,  0.4745,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [ 0.4824,  0.4745,  0.4588,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.6471, -0.6471, -0.6471,  ..., -0.9137, -0.9137, -0.9137]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4353,  0.4275,  0.4039,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [ 0.4196,  0.4118,  0.3961,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [ 0.4039,  0.3961,  0.3804,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          ...,\n",
      "          [-0.7255, -0.7255, -0.7333,  ..., -0.9059, -0.9059, -0.9059],\n",
      "          [-0.7098, -0.7098, -0.7176,  ..., -0.9059, -0.9059, -0.9059],\n",
      "          [-0.7020, -0.7020, -0.7098,  ..., -0.9059, -0.9059, -0.9059]],\n",
      "\n",
      "         [[ 0.5294,  0.5216,  0.4980,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          [ 0.5137,  0.5059,  0.4902,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          [ 0.4980,  0.4902,  0.4745,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.6471, -0.6471, -0.6471,  ..., -0.8902, -0.8902, -0.8902]],\n",
      "\n",
      "         [[ 0.5137,  0.5059,  0.4824,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [ 0.4980,  0.4902,  0.4745,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [ 0.4824,  0.4745,  0.4588,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.6471, -0.6471, -0.6471,  ..., -0.9137, -0.9137, -0.9137]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.4353,  0.4275,  0.4039,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [ 0.4196,  0.4118,  0.3961,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [ 0.4039,  0.3961,  0.3804,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          ...,\n",
      "          [-0.7255, -0.7255, -0.7333,  ..., -0.9059, -0.9059, -0.9059],\n",
      "          [-0.7098, -0.7098, -0.7176,  ..., -0.9059, -0.9059, -0.9059],\n",
      "          [-0.7020, -0.7020, -0.7098,  ..., -0.9059, -0.9059, -0.9059]],\n",
      "\n",
      "         [[ 0.5294,  0.5216,  0.4980,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          [ 0.5137,  0.5059,  0.4902,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          [ 0.4980,  0.4902,  0.4745,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.6471, -0.6471, -0.6471,  ..., -0.8902, -0.8902, -0.8902]],\n",
      "\n",
      "         [[ 0.5137,  0.5059,  0.4824,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [ 0.4980,  0.4902,  0.4745,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [ 0.4824,  0.4745,  0.4588,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.6471, -0.6471, -0.6471,  ..., -0.9137, -0.9137, -0.9137]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4353,  0.4275,  0.4039,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [ 0.4196,  0.4118,  0.3961,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          [ 0.4039,  0.3961,  0.3804,  ..., -0.5922, -0.5922, -0.5922],\n",
      "          ...,\n",
      "          [-0.7255, -0.7255, -0.7333,  ..., -0.9059, -0.9059, -0.9059],\n",
      "          [-0.7098, -0.7098, -0.7176,  ..., -0.9059, -0.9059, -0.9059],\n",
      "          [-0.7020, -0.7020, -0.7098,  ..., -0.9059, -0.9059, -0.9059]],\n",
      "\n",
      "         [[ 0.5294,  0.5216,  0.4980,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          [ 0.5137,  0.5059,  0.4902,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          [ 0.4980,  0.4902,  0.4745,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.6471, -0.6471, -0.6471,  ..., -0.8902, -0.8902, -0.8902]],\n",
      "\n",
      "         [[ 0.5137,  0.5059,  0.4824,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [ 0.4980,  0.4902,  0.4745,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [ 0.4824,  0.4745,  0.4588,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6863,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.6627, -0.6627, -0.6627,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.6471, -0.6471, -0.6471,  ..., -0.9137, -0.9137, -0.9137]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.2549,  0.2549,  0.2471,  ...,  0.3804,  0.3804,  0.3804],\n",
      "          [ 0.2627,  0.2627,  0.2549,  ...,  0.3804,  0.3882,  0.3882],\n",
      "          [ 0.2941,  0.2941,  0.2784,  ...,  0.3961,  0.4039,  0.4039],\n",
      "          ...,\n",
      "          [-0.7490, -0.7333, -0.7098,  ..., -0.1137, -0.1137, -0.1137],\n",
      "          [-0.7333, -0.7176, -0.6941,  ..., -0.0588, -0.0588, -0.0588],\n",
      "          [-0.7255, -0.7098, -0.6863,  ..., -0.0275, -0.0275, -0.0275]],\n",
      "\n",
      "         [[ 0.2706,  0.2706,  0.2784,  ...,  0.1216,  0.1137,  0.1137],\n",
      "          [ 0.2941,  0.2941,  0.2863,  ...,  0.1373,  0.1294,  0.1294],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ...,  0.1608,  0.1529,  0.1529],\n",
      "          ...,\n",
      "          [-0.8039, -0.7882, -0.7647,  ...,  0.2314,  0.2314,  0.2314],\n",
      "          [-0.7882, -0.7725, -0.7490,  ...,  0.2863,  0.2863,  0.2863],\n",
      "          [-0.7804, -0.7647, -0.7412,  ...,  0.3176,  0.3176,  0.3176]],\n",
      "\n",
      "         [[-0.2784, -0.2784, -0.2784,  ..., -0.1373, -0.1451, -0.1451],\n",
      "          [-0.2706, -0.2706, -0.2784,  ..., -0.1216, -0.1294, -0.1294],\n",
      "          [-0.2549, -0.2549, -0.2706,  ..., -0.0980, -0.0902, -0.0902],\n",
      "          ...,\n",
      "          [-0.9373, -0.9294, -0.9059,  ..., -0.5137, -0.5137, -0.5137],\n",
      "          [-0.9294, -0.9216, -0.8902,  ..., -0.4588, -0.4588, -0.4588],\n",
      "          [-0.9216, -0.9137, -0.8824,  ..., -0.4275, -0.4275, -0.4275]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2549,  0.2549,  0.2471,  ...,  0.3804,  0.3804,  0.3804],\n",
      "          [ 0.2627,  0.2627,  0.2549,  ...,  0.3804,  0.3882,  0.3882],\n",
      "          [ 0.2941,  0.2941,  0.2784,  ...,  0.3961,  0.4039,  0.4039],\n",
      "          ...,\n",
      "          [-0.7490, -0.7333, -0.7098,  ..., -0.1137, -0.1137, -0.1137],\n",
      "          [-0.7333, -0.7176, -0.6941,  ..., -0.0588, -0.0588, -0.0588],\n",
      "          [-0.7255, -0.7098, -0.6863,  ..., -0.0275, -0.0275, -0.0275]],\n",
      "\n",
      "         [[ 0.2706,  0.2706,  0.2784,  ...,  0.1216,  0.1137,  0.1137],\n",
      "          [ 0.2941,  0.2941,  0.2863,  ...,  0.1373,  0.1294,  0.1294],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ...,  0.1608,  0.1529,  0.1529],\n",
      "          ...,\n",
      "          [-0.8039, -0.7882, -0.7647,  ...,  0.2314,  0.2314,  0.2314],\n",
      "          [-0.7882, -0.7725, -0.7490,  ...,  0.2863,  0.2863,  0.2863],\n",
      "          [-0.7804, -0.7647, -0.7412,  ...,  0.3176,  0.3176,  0.3176]],\n",
      "\n",
      "         [[-0.2784, -0.2784, -0.2784,  ..., -0.1373, -0.1451, -0.1451],\n",
      "          [-0.2706, -0.2706, -0.2784,  ..., -0.1216, -0.1294, -0.1294],\n",
      "          [-0.2549, -0.2549, -0.2706,  ..., -0.0980, -0.0902, -0.0902],\n",
      "          ...,\n",
      "          [-0.9373, -0.9294, -0.9059,  ..., -0.5137, -0.5137, -0.5137],\n",
      "          [-0.9294, -0.9216, -0.8902,  ..., -0.4588, -0.4588, -0.4588],\n",
      "          [-0.9216, -0.9137, -0.8824,  ..., -0.4275, -0.4275, -0.4275]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.2549,  0.2549,  0.2471,  ...,  0.3804,  0.3804,  0.3804],\n",
      "          [ 0.2627,  0.2627,  0.2549,  ...,  0.3804,  0.3882,  0.3882],\n",
      "          [ 0.2941,  0.2941,  0.2784,  ...,  0.3961,  0.4039,  0.4039],\n",
      "          ...,\n",
      "          [-0.7490, -0.7333, -0.7098,  ..., -0.1137, -0.1137, -0.1137],\n",
      "          [-0.7333, -0.7176, -0.6941,  ..., -0.0588, -0.0588, -0.0588],\n",
      "          [-0.7255, -0.7098, -0.6863,  ..., -0.0275, -0.0275, -0.0275]],\n",
      "\n",
      "         [[ 0.2706,  0.2706,  0.2784,  ...,  0.1216,  0.1137,  0.1137],\n",
      "          [ 0.2941,  0.2941,  0.2863,  ...,  0.1373,  0.1294,  0.1294],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ...,  0.1608,  0.1529,  0.1529],\n",
      "          ...,\n",
      "          [-0.8039, -0.7882, -0.7647,  ...,  0.2314,  0.2314,  0.2314],\n",
      "          [-0.7882, -0.7725, -0.7490,  ...,  0.2863,  0.2863,  0.2863],\n",
      "          [-0.7804, -0.7647, -0.7412,  ...,  0.3176,  0.3176,  0.3176]],\n",
      "\n",
      "         [[-0.2784, -0.2784, -0.2784,  ..., -0.1373, -0.1451, -0.1451],\n",
      "          [-0.2706, -0.2706, -0.2784,  ..., -0.1216, -0.1294, -0.1294],\n",
      "          [-0.2549, -0.2549, -0.2706,  ..., -0.0980, -0.0902, -0.0902],\n",
      "          ...,\n",
      "          [-0.9373, -0.9294, -0.9059,  ..., -0.5137, -0.5137, -0.5137],\n",
      "          [-0.9294, -0.9216, -0.8902,  ..., -0.4588, -0.4588, -0.4588],\n",
      "          [-0.9216, -0.9137, -0.8824,  ..., -0.4275, -0.4275, -0.4275]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2549,  0.2549,  0.2471,  ...,  0.3804,  0.3804,  0.3804],\n",
      "          [ 0.2627,  0.2627,  0.2549,  ...,  0.3804,  0.3882,  0.3882],\n",
      "          [ 0.2941,  0.2941,  0.2784,  ...,  0.3961,  0.4039,  0.4039],\n",
      "          ...,\n",
      "          [-0.7490, -0.7333, -0.7098,  ..., -0.1137, -0.1137, -0.1137],\n",
      "          [-0.7333, -0.7176, -0.6941,  ..., -0.0588, -0.0588, -0.0588],\n",
      "          [-0.7255, -0.7098, -0.6863,  ..., -0.0275, -0.0275, -0.0275]],\n",
      "\n",
      "         [[ 0.2706,  0.2706,  0.2784,  ...,  0.1216,  0.1137,  0.1137],\n",
      "          [ 0.2941,  0.2941,  0.2863,  ...,  0.1373,  0.1294,  0.1294],\n",
      "          [ 0.3176,  0.3176,  0.3098,  ...,  0.1608,  0.1529,  0.1529],\n",
      "          ...,\n",
      "          [-0.8039, -0.7882, -0.7647,  ...,  0.2314,  0.2314,  0.2314],\n",
      "          [-0.7882, -0.7725, -0.7490,  ...,  0.2863,  0.2863,  0.2863],\n",
      "          [-0.7804, -0.7647, -0.7412,  ...,  0.3176,  0.3176,  0.3176]],\n",
      "\n",
      "         [[-0.2784, -0.2784, -0.2784,  ..., -0.1373, -0.1451, -0.1451],\n",
      "          [-0.2706, -0.2706, -0.2784,  ..., -0.1216, -0.1294, -0.1294],\n",
      "          [-0.2549, -0.2549, -0.2706,  ..., -0.0980, -0.0902, -0.0902],\n",
      "          ...,\n",
      "          [-0.9373, -0.9294, -0.9059,  ..., -0.5137, -0.5137, -0.5137],\n",
      "          [-0.9294, -0.9216, -0.8902,  ..., -0.4588, -0.4588, -0.4588],\n",
      "          [-0.9216, -0.9137, -0.8824,  ..., -0.4275, -0.4275, -0.4275]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.5608,  0.5608,  0.5608,  ...,  0.7255,  0.7490,  0.7490],\n",
      "          [ 0.5608,  0.5608,  0.5608,  ...,  0.7333,  0.7569,  0.7490],\n",
      "          [ 0.5608,  0.5608,  0.5608,  ...,  0.7412,  0.7647,  0.7647],\n",
      "          ...,\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431],\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431],\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431]],\n",
      "\n",
      "         [[ 0.5765,  0.5765,  0.5765,  ...,  0.0353,  0.0824,  0.1059],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.0431,  0.0824,  0.1059],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.0510,  0.0980,  0.1216],\n",
      "          ...,\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667]],\n",
      "\n",
      "         [[ 0.6784,  0.6784,  0.6784,  ..., -0.0510, -0.0039,  0.0196],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ..., -0.0431, -0.0039,  0.0196],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ..., -0.0353,  0.0118,  0.0353],\n",
      "          ...,\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118],\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118],\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5608,  0.5608,  0.5608,  ...,  0.7255,  0.7490,  0.7490],\n",
      "          [ 0.5608,  0.5608,  0.5608,  ...,  0.7333,  0.7569,  0.7490],\n",
      "          [ 0.5608,  0.5608,  0.5608,  ...,  0.7412,  0.7647,  0.7647],\n",
      "          ...,\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431],\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431],\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431]],\n",
      "\n",
      "         [[ 0.5765,  0.5765,  0.5765,  ...,  0.0353,  0.0824,  0.1059],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.0431,  0.0824,  0.1059],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.0510,  0.0980,  0.1216],\n",
      "          ...,\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667]],\n",
      "\n",
      "         [[ 0.6784,  0.6784,  0.6784,  ..., -0.0510, -0.0039,  0.0196],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ..., -0.0431, -0.0039,  0.0196],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ..., -0.0353,  0.0118,  0.0353],\n",
      "          ...,\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118],\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118],\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.5608,  0.5608,  0.5608,  ...,  0.7255,  0.7490,  0.7490],\n",
      "          [ 0.5608,  0.5608,  0.5608,  ...,  0.7333,  0.7569,  0.7490],\n",
      "          [ 0.5608,  0.5608,  0.5608,  ...,  0.7412,  0.7647,  0.7647],\n",
      "          ...,\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431],\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431],\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431]],\n",
      "\n",
      "         [[ 0.5765,  0.5765,  0.5765,  ...,  0.0353,  0.0824,  0.1059],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.0431,  0.0824,  0.1059],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.0510,  0.0980,  0.1216],\n",
      "          ...,\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667]],\n",
      "\n",
      "         [[ 0.6784,  0.6784,  0.6784,  ..., -0.0510, -0.0039,  0.0196],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ..., -0.0431, -0.0039,  0.0196],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ..., -0.0353,  0.0118,  0.0353],\n",
      "          ...,\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118],\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118],\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5608,  0.5608,  0.5608,  ...,  0.7255,  0.7490,  0.7490],\n",
      "          [ 0.5608,  0.5608,  0.5608,  ...,  0.7333,  0.7569,  0.7490],\n",
      "          [ 0.5608,  0.5608,  0.5608,  ...,  0.7412,  0.7647,  0.7647],\n",
      "          ...,\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431],\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431],\n",
      "          [ 0.2471,  0.2471,  0.2314,  ...,  0.4431,  0.4431,  0.4431]],\n",
      "\n",
      "         [[ 0.5765,  0.5765,  0.5765,  ...,  0.0353,  0.0824,  0.1059],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.0431,  0.0824,  0.1059],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.0510,  0.0980,  0.1216],\n",
      "          ...,\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667],\n",
      "          [ 0.2706,  0.2706,  0.2549,  ...,  0.4667,  0.4667,  0.4667]],\n",
      "\n",
      "         [[ 0.6784,  0.6784,  0.6784,  ..., -0.0510, -0.0039,  0.0196],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ..., -0.0431, -0.0039,  0.0196],\n",
      "          [ 0.6784,  0.6784,  0.6784,  ..., -0.0353,  0.0118,  0.0353],\n",
      "          ...,\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118],\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118],\n",
      "          [ 0.2157,  0.2157,  0.2000,  ...,  0.4118,  0.4118,  0.4118]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          ...,\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5843, -0.5451],\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5765, -0.5373],\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5686, -0.5294]],\n",
      "\n",
      "         [[-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6392, -0.6000],\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6314, -0.5922],\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6235, -0.5843]],\n",
      "\n",
      "         [[-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          ...,\n",
      "          [-0.6392, -0.6471, -0.6706,  ..., -0.8196, -0.7176, -0.6784],\n",
      "          [-0.6392, -0.6471, -0.6784,  ..., -0.8196, -0.7098, -0.6706],\n",
      "          [-0.6392, -0.6471, -0.6863,  ..., -0.8196, -0.7020, -0.6627]]],\n",
      "\n",
      "\n",
      "        [[[-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          ...,\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5843, -0.5451],\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5765, -0.5373],\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5686, -0.5294]],\n",
      "\n",
      "         [[-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6392, -0.6000],\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6314, -0.5922],\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6235, -0.5843]],\n",
      "\n",
      "         [[-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          ...,\n",
      "          [-0.6392, -0.6471, -0.6706,  ..., -0.8196, -0.7176, -0.6784],\n",
      "          [-0.6392, -0.6471, -0.6784,  ..., -0.8196, -0.7098, -0.6706],\n",
      "          [-0.6392, -0.6471, -0.6863,  ..., -0.8196, -0.7020, -0.6627]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          ...,\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5843, -0.5451],\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5765, -0.5373],\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5686, -0.5294]],\n",
      "\n",
      "         [[-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6392, -0.6000],\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6314, -0.5922],\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6235, -0.5843]],\n",
      "\n",
      "         [[-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          ...,\n",
      "          [-0.6392, -0.6471, -0.6706,  ..., -0.8196, -0.7176, -0.6784],\n",
      "          [-0.6392, -0.6471, -0.6784,  ..., -0.8196, -0.7098, -0.6706],\n",
      "          [-0.6392, -0.6471, -0.6863,  ..., -0.8196, -0.7020, -0.6627]]],\n",
      "\n",
      "\n",
      "        [[[-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [-0.9922, -0.9922, -0.9922,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          ...,\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5843, -0.5451],\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5765, -0.5373],\n",
      "          [-0.8902, -0.8980, -0.9294,  ..., -0.6863, -0.5686, -0.5294]],\n",
      "\n",
      "         [[-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.9529, -0.9529, -0.9529,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6392, -0.6000],\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6314, -0.5922],\n",
      "          [-0.7882, -0.7961, -0.8275,  ..., -0.7412, -0.6235, -0.5843]],\n",
      "\n",
      "         [[-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [-0.7961, -0.7961, -0.7961,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          ...,\n",
      "          [-0.6392, -0.6471, -0.6706,  ..., -0.8196, -0.7176, -0.6784],\n",
      "          [-0.6392, -0.6471, -0.6784,  ..., -0.8196, -0.7098, -0.6706],\n",
      "          [-0.6392, -0.6471, -0.6863,  ..., -0.8196, -0.7020, -0.6627]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.1294,  0.1294,  0.1294,  ...,  0.3647,  0.3647,  0.3647],\n",
      "          [ 0.1294,  0.1294,  0.1294,  ...,  0.3725,  0.3725,  0.3725],\n",
      "          [ 0.1294,  0.1294,  0.1294,  ...,  0.3725,  0.3725,  0.3725],\n",
      "          ...,\n",
      "          [ 0.9373,  0.9137,  0.7020,  ..., -0.8353, -0.8196, -0.8196],\n",
      "          [ 1.0000,  1.0000,  0.7490,  ..., -0.8353, -0.8196, -0.8196],\n",
      "          [ 1.0000,  1.0000,  0.7804,  ..., -0.8431, -0.8196, -0.8196]],\n",
      "\n",
      "         [[ 0.5765,  0.5765,  0.5765,  ...,  0.6392,  0.6392,  0.6392],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.6471,  0.6471,  0.6471],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.6471,  0.6471,  0.6471],\n",
      "          ...,\n",
      "          [ 0.8667,  0.7804,  0.5608,  ..., -0.8196, -0.8039, -0.8039],\n",
      "          [ 0.9922,  0.8902,  0.6235,  ..., -0.8196, -0.8039, -0.8039],\n",
      "          [ 1.0000,  0.9373,  0.6471,  ..., -0.8275, -0.8039, -0.8039]],\n",
      "\n",
      "         [[ 0.9294,  0.9294,  0.9294,  ...,  0.8745,  0.8745,  0.8745],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8824,  0.8824,  0.8824],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8824,  0.8824,  0.8824],\n",
      "          ...,\n",
      "          [ 0.8118,  0.7255,  0.4980,  ..., -0.8431, -0.8275, -0.8275],\n",
      "          [ 0.9373,  0.8353,  0.5608,  ..., -0.8431, -0.8275, -0.8275],\n",
      "          [ 1.0000,  0.8824,  0.5843,  ..., -0.8510, -0.8275, -0.8275]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1294,  0.1294,  0.1294,  ...,  0.3647,  0.3647,  0.3647],\n",
      "          [ 0.1294,  0.1294,  0.1294,  ...,  0.3725,  0.3725,  0.3725],\n",
      "          [ 0.1294,  0.1294,  0.1294,  ...,  0.3725,  0.3725,  0.3725],\n",
      "          ...,\n",
      "          [ 0.9373,  0.9137,  0.7020,  ..., -0.8353, -0.8196, -0.8196],\n",
      "          [ 1.0000,  1.0000,  0.7490,  ..., -0.8353, -0.8196, -0.8196],\n",
      "          [ 1.0000,  1.0000,  0.7804,  ..., -0.8431, -0.8196, -0.8196]],\n",
      "\n",
      "         [[ 0.5765,  0.5765,  0.5765,  ...,  0.6392,  0.6392,  0.6392],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.6471,  0.6471,  0.6471],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.6471,  0.6471,  0.6471],\n",
      "          ...,\n",
      "          [ 0.8667,  0.7804,  0.5608,  ..., -0.8196, -0.8039, -0.8039],\n",
      "          [ 0.9922,  0.8902,  0.6235,  ..., -0.8196, -0.8039, -0.8039],\n",
      "          [ 1.0000,  0.9373,  0.6471,  ..., -0.8275, -0.8039, -0.8039]],\n",
      "\n",
      "         [[ 0.9294,  0.9294,  0.9294,  ...,  0.8745,  0.8745,  0.8745],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8824,  0.8824,  0.8824],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8824,  0.8824,  0.8824],\n",
      "          ...,\n",
      "          [ 0.8118,  0.7255,  0.4980,  ..., -0.8431, -0.8275, -0.8275],\n",
      "          [ 0.9373,  0.8353,  0.5608,  ..., -0.8431, -0.8275, -0.8275],\n",
      "          [ 1.0000,  0.8824,  0.5843,  ..., -0.8510, -0.8275, -0.8275]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.1294,  0.1294,  0.1294,  ...,  0.3647,  0.3647,  0.3647],\n",
      "          [ 0.1294,  0.1294,  0.1294,  ...,  0.3725,  0.3725,  0.3725],\n",
      "          [ 0.1294,  0.1294,  0.1294,  ...,  0.3725,  0.3725,  0.3725],\n",
      "          ...,\n",
      "          [ 0.9373,  0.9137,  0.7020,  ..., -0.8353, -0.8196, -0.8196],\n",
      "          [ 1.0000,  1.0000,  0.7490,  ..., -0.8353, -0.8196, -0.8196],\n",
      "          [ 1.0000,  1.0000,  0.7804,  ..., -0.8431, -0.8196, -0.8196]],\n",
      "\n",
      "         [[ 0.5765,  0.5765,  0.5765,  ...,  0.6392,  0.6392,  0.6392],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.6471,  0.6471,  0.6471],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.6471,  0.6471,  0.6471],\n",
      "          ...,\n",
      "          [ 0.8667,  0.7804,  0.5608,  ..., -0.8196, -0.8039, -0.8039],\n",
      "          [ 0.9922,  0.8902,  0.6235,  ..., -0.8196, -0.8039, -0.8039],\n",
      "          [ 1.0000,  0.9373,  0.6471,  ..., -0.8275, -0.8039, -0.8039]],\n",
      "\n",
      "         [[ 0.9294,  0.9294,  0.9294,  ...,  0.8745,  0.8745,  0.8745],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8824,  0.8824,  0.8824],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8824,  0.8824,  0.8824],\n",
      "          ...,\n",
      "          [ 0.8118,  0.7255,  0.4980,  ..., -0.8431, -0.8275, -0.8275],\n",
      "          [ 0.9373,  0.8353,  0.5608,  ..., -0.8431, -0.8275, -0.8275],\n",
      "          [ 1.0000,  0.8824,  0.5843,  ..., -0.8510, -0.8275, -0.8275]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1294,  0.1294,  0.1294,  ...,  0.3647,  0.3647,  0.3647],\n",
      "          [ 0.1294,  0.1294,  0.1294,  ...,  0.3725,  0.3725,  0.3725],\n",
      "          [ 0.1294,  0.1294,  0.1294,  ...,  0.3725,  0.3725,  0.3725],\n",
      "          ...,\n",
      "          [ 0.9373,  0.9137,  0.7020,  ..., -0.8353, -0.8196, -0.8196],\n",
      "          [ 1.0000,  1.0000,  0.7490,  ..., -0.8353, -0.8196, -0.8196],\n",
      "          [ 1.0000,  1.0000,  0.7804,  ..., -0.8431, -0.8196, -0.8196]],\n",
      "\n",
      "         [[ 0.5765,  0.5765,  0.5765,  ...,  0.6392,  0.6392,  0.6392],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.6471,  0.6471,  0.6471],\n",
      "          [ 0.5765,  0.5765,  0.5765,  ...,  0.6471,  0.6471,  0.6471],\n",
      "          ...,\n",
      "          [ 0.8667,  0.7804,  0.5608,  ..., -0.8196, -0.8039, -0.8039],\n",
      "          [ 0.9922,  0.8902,  0.6235,  ..., -0.8196, -0.8039, -0.8039],\n",
      "          [ 1.0000,  0.9373,  0.6471,  ..., -0.8275, -0.8039, -0.8039]],\n",
      "\n",
      "         [[ 0.9294,  0.9294,  0.9294,  ...,  0.8745,  0.8745,  0.8745],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8824,  0.8824,  0.8824],\n",
      "          [ 0.9294,  0.9294,  0.9294,  ...,  0.8824,  0.8824,  0.8824],\n",
      "          ...,\n",
      "          [ 0.8118,  0.7255,  0.4980,  ..., -0.8431, -0.8275, -0.8275],\n",
      "          [ 0.9373,  0.8353,  0.5608,  ..., -0.8431, -0.8275, -0.8275],\n",
      "          [ 1.0000,  0.8824,  0.5843,  ..., -0.8510, -0.8275, -0.8275]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8510, -0.8510],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8510, -0.8510],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8588, -0.8588],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9294, -0.9216, -0.9216],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9216, -0.9059, -0.9059]],\n",
      "\n",
      "         [[-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8745, -0.8745],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8745, -0.8745],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8824, -0.8824],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9373, -0.9294, -0.9294],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9294, -0.9137, -0.9137]],\n",
      "\n",
      "         [[-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9216, -0.9216],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9765, -0.9686, -0.9686],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9686, -0.9529, -0.9529]]],\n",
      "\n",
      "\n",
      "        [[[-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8510, -0.8510],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8510, -0.8510],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8588, -0.8588],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9294, -0.9216, -0.9216],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9216, -0.9059, -0.9059]],\n",
      "\n",
      "         [[-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8745, -0.8745],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8745, -0.8745],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8824, -0.8824],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9373, -0.9294, -0.9294],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9294, -0.9137, -0.9137]],\n",
      "\n",
      "         [[-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9216, -0.9216],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9765, -0.9686, -0.9686],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9686, -0.9529, -0.9529]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8510, -0.8510],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8510, -0.8510],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8588, -0.8588],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9294, -0.9216, -0.9216],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9216, -0.9059, -0.9059]],\n",
      "\n",
      "         [[-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8745, -0.8745],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8745, -0.8745],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8824, -0.8824],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9373, -0.9294, -0.9294],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9294, -0.9137, -0.9137]],\n",
      "\n",
      "         [[-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9216, -0.9216],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9765, -0.9686, -0.9686],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9686, -0.9529, -0.9529]]],\n",
      "\n",
      "\n",
      "        [[[-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8510, -0.8510],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8510, -0.8510],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8510, -0.8588, -0.8588],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -0.9529, -0.9529, -0.9529],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9294, -0.9216, -0.9216],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9216, -0.9059, -0.9059]],\n",
      "\n",
      "         [[-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8745, -0.8745],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8745, -0.8745],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.8745, -0.8824, -0.8824],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -0.9608, -0.9608, -0.9608],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9373, -0.9294, -0.9294],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9294, -0.9137, -0.9137]],\n",
      "\n",
      "         [[-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9137, -0.9137],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9137, -0.9216, -0.9216],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9686,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9765, -0.9686, -0.9686],\n",
      "          [-0.9529, -0.9529, -0.9608,  ..., -0.9686, -0.9529, -0.9529]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.0510, -0.0588, -0.0902,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0510, -0.0588, -0.0902,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0431, -0.0510, -0.0824,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667],\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667],\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667]],\n",
      "\n",
      "         [[-0.0353, -0.0431, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0353, -0.0431, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0275, -0.0353, -0.0667,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588],\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588],\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588]],\n",
      "\n",
      "         [[-0.0745, -0.0824, -0.1137,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0745, -0.0824, -0.1137,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0667, -0.0745, -0.1059,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431],\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431],\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431]]],\n",
      "\n",
      "\n",
      "        [[[-0.0510, -0.0588, -0.0902,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0510, -0.0588, -0.0902,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0431, -0.0510, -0.0824,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667],\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667],\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667]],\n",
      "\n",
      "         [[-0.0353, -0.0431, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0353, -0.0431, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0275, -0.0353, -0.0667,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588],\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588],\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588]],\n",
      "\n",
      "         [[-0.0745, -0.0824, -0.1137,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0745, -0.0824, -0.1137,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0667, -0.0745, -0.1059,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431],\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431],\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.0510, -0.0588, -0.0902,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0510, -0.0588, -0.0902,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0431, -0.0510, -0.0824,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667],\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667],\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667]],\n",
      "\n",
      "         [[-0.0353, -0.0431, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0353, -0.0431, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0275, -0.0353, -0.0667,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588],\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588],\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588]],\n",
      "\n",
      "         [[-0.0745, -0.0824, -0.1137,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0745, -0.0824, -0.1137,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0667, -0.0745, -0.1059,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431],\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431],\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431]]],\n",
      "\n",
      "\n",
      "        [[[-0.0510, -0.0588, -0.0902,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0510, -0.0588, -0.0902,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0431, -0.0510, -0.0824,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667],\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667],\n",
      "          [-0.4431, -0.4431, -0.4510,  ..., -0.8431, -0.8588, -0.8667]],\n",
      "\n",
      "         [[-0.0353, -0.0431, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0353, -0.0431, -0.0745,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0275, -0.0353, -0.0667,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588],\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588],\n",
      "          [-0.4118, -0.4118, -0.4196,  ..., -0.8353, -0.8510, -0.8588]],\n",
      "\n",
      "         [[-0.0745, -0.0824, -0.1137,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0745, -0.0824, -0.1137,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [-0.0667, -0.0745, -0.1059,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431],\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431],\n",
      "          [-0.6863, -0.6863, -0.6941,  ..., -0.8196, -0.8353, -0.8431]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          ...,\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          [-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          [-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          ...,\n",
      "          [-0.8902, -0.8902, -0.8902,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.8824, -0.8824, -0.8824,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.8824, -0.8824, -0.8824,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          ...,\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          [-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          [-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          ...,\n",
      "          [-0.8902, -0.8902, -0.8902,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.8824, -0.8824, -0.8824,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.8824, -0.8824, -0.8824,  ..., -1.0000, -1.0000, -1.0000]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          ...,\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          [-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          [-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          ...,\n",
      "          [-0.8902, -0.8902, -0.8902,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.8824, -0.8824, -0.8824,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.8824, -0.8824, -0.8824,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.9216, -0.9216, -0.9216],\n",
      "          ...,\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9608, -0.9608, -0.9608,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          [-0.9294, -0.9294, -0.9294,  ..., -0.8902, -0.8902, -0.8902],\n",
      "          ...,\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9373, -0.9373, -0.9373,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          [-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          [-0.9137, -0.9137, -0.9137,  ..., -0.8667, -0.8667, -0.8667],\n",
      "          ...,\n",
      "          [-0.8902, -0.8902, -0.8902,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.8824, -0.8824, -0.8824,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.8824, -0.8824, -0.8824,  ..., -1.0000, -1.0000, -1.0000]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          [-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          [-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          ...,\n",
      "          [-0.0667, -0.0588, -0.0196,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.0667, -0.0588, -0.0275,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.0667, -0.0588, -0.0353,  ..., -0.5686, -0.5608, -0.5608]],\n",
      "\n",
      "         [[-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          [-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          [-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          ...,\n",
      "          [-0.4275, -0.4118, -0.3725,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.4353, -0.4275, -0.3882,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.4431, -0.4353, -0.3961,  ..., -0.5686, -0.5608, -0.5608]],\n",
      "\n",
      "         [[-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          [-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          [-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          ...,\n",
      "          [-0.6392, -0.6235, -0.5765,  ..., -0.5686, -0.5686, -0.5686],\n",
      "          [-0.6549, -0.6392, -0.6000,  ..., -0.5765, -0.5765, -0.5765],\n",
      "          [-0.6627, -0.6471, -0.6157,  ..., -0.5765, -0.5765, -0.5765]]],\n",
      "\n",
      "\n",
      "        [[[-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          [-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          [-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          ...,\n",
      "          [-0.0667, -0.0588, -0.0196,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.0667, -0.0588, -0.0275,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.0667, -0.0588, -0.0353,  ..., -0.5686, -0.5608, -0.5608]],\n",
      "\n",
      "         [[-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          [-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          [-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          ...,\n",
      "          [-0.4275, -0.4118, -0.3725,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.4353, -0.4275, -0.3882,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.4431, -0.4353, -0.3961,  ..., -0.5686, -0.5608, -0.5608]],\n",
      "\n",
      "         [[-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          [-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          [-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          ...,\n",
      "          [-0.6392, -0.6235, -0.5765,  ..., -0.5686, -0.5686, -0.5686],\n",
      "          [-0.6549, -0.6392, -0.6000,  ..., -0.5765, -0.5765, -0.5765],\n",
      "          [-0.6627, -0.6471, -0.6157,  ..., -0.5765, -0.5765, -0.5765]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          [-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          [-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          ...,\n",
      "          [-0.0667, -0.0588, -0.0196,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.0667, -0.0588, -0.0275,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.0667, -0.0588, -0.0353,  ..., -0.5686, -0.5608, -0.5608]],\n",
      "\n",
      "         [[-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          [-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          [-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          ...,\n",
      "          [-0.4275, -0.4118, -0.3725,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.4353, -0.4275, -0.3882,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.4431, -0.4353, -0.3961,  ..., -0.5686, -0.5608, -0.5608]],\n",
      "\n",
      "         [[-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          [-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          [-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          ...,\n",
      "          [-0.6392, -0.6235, -0.5765,  ..., -0.5686, -0.5686, -0.5686],\n",
      "          [-0.6549, -0.6392, -0.6000,  ..., -0.5765, -0.5765, -0.5765],\n",
      "          [-0.6627, -0.6471, -0.6157,  ..., -0.5765, -0.5765, -0.5765]]],\n",
      "\n",
      "\n",
      "        [[[-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          [-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          [-0.9216, -0.9216, -0.9059,  ..., -0.7490, -0.7255, -0.7176],\n",
      "          ...,\n",
      "          [-0.0667, -0.0588, -0.0196,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.0667, -0.0588, -0.0275,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.0667, -0.0588, -0.0353,  ..., -0.5686, -0.5608, -0.5608]],\n",
      "\n",
      "         [[-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          [-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          [-0.8902, -0.8902, -0.8745,  ..., -0.7176, -0.7020, -0.6941],\n",
      "          ...,\n",
      "          [-0.4275, -0.4118, -0.3725,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.4353, -0.4275, -0.3882,  ..., -0.5686, -0.5608, -0.5608],\n",
      "          [-0.4431, -0.4353, -0.3961,  ..., -0.5686, -0.5608, -0.5608]],\n",
      "\n",
      "         [[-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          [-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          [-0.8667, -0.8667, -0.8510,  ..., -0.8039, -0.7882, -0.7882],\n",
      "          ...,\n",
      "          [-0.6392, -0.6235, -0.5765,  ..., -0.5686, -0.5686, -0.5686],\n",
      "          [-0.6549, -0.6392, -0.6000,  ..., -0.5765, -0.5765, -0.5765],\n",
      "          [-0.6627, -0.6471, -0.6157,  ..., -0.5765, -0.5765, -0.5765]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          ...,\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2706,  0.3176,  0.3333],\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2314,  0.2941,  0.3098],\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2235,  0.2784,  0.2941]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          ...,\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.1373,  0.1843,  0.2000],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.0980,  0.1608,  0.1765],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.0902,  0.1451,  0.1686]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5451,  0.5373,  ..., -0.0275,  0.0275,  0.0353],\n",
      "          [ 0.5373,  0.5373,  0.5373,  ..., -0.0510,  0.0039,  0.0196],\n",
      "          [ 0.5373,  0.5373,  0.5373,  ..., -0.0667, -0.0039,  0.0118]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          ...,\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2706,  0.3176,  0.3333],\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2314,  0.2941,  0.3098],\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2235,  0.2784,  0.2941]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          ...,\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.1373,  0.1843,  0.2000],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.0980,  0.1608,  0.1765],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.0902,  0.1451,  0.1686]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5451,  0.5373,  ..., -0.0275,  0.0275,  0.0353],\n",
      "          [ 0.5373,  0.5373,  0.5373,  ..., -0.0510,  0.0039,  0.0196],\n",
      "          [ 0.5373,  0.5373,  0.5373,  ..., -0.0667, -0.0039,  0.0118]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          ...,\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2706,  0.3176,  0.3333],\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2314,  0.2941,  0.3098],\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2235,  0.2784,  0.2941]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          ...,\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.1373,  0.1843,  0.2000],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.0980,  0.1608,  0.1765],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.0902,  0.1451,  0.1686]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5451,  0.5373,  ..., -0.0275,  0.0275,  0.0353],\n",
      "          [ 0.5373,  0.5373,  0.5373,  ..., -0.0510,  0.0039,  0.0196],\n",
      "          [ 0.5373,  0.5373,  0.5373,  ..., -0.0667, -0.0039,  0.0118]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          ...,\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2706,  0.3176,  0.3333],\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2314,  0.2941,  0.3098],\n",
      "          [ 0.6157,  0.6157,  0.6157,  ...,  0.2235,  0.2784,  0.2941]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          ...,\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.1373,  0.1843,  0.2000],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.0980,  0.1608,  0.1765],\n",
      "          [ 0.5922,  0.5922,  0.5922,  ...,  0.0902,  0.1451,  0.1686]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5451,  0.5373,  ..., -0.0275,  0.0275,  0.0353],\n",
      "          [ 0.5373,  0.5373,  0.5373,  ..., -0.0510,  0.0039,  0.0196],\n",
      "          [ 0.5373,  0.5373,  0.5373,  ..., -0.0667, -0.0039,  0.0118]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          [-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          [-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          ...,\n",
      "          [ 0.5922,  0.5843,  0.5686,  ..., -0.5608, -0.5608, -0.5608],\n",
      "          [ 0.5922,  0.5843,  0.5686,  ..., -0.5765, -0.5765, -0.5765],\n",
      "          [ 0.5922,  0.5765,  0.5608,  ..., -0.5843, -0.5843, -0.5843]],\n",
      "\n",
      "         [[-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          ...,\n",
      "          [ 0.5216,  0.5137,  0.4980,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [ 0.5216,  0.5137,  0.4980,  ..., -0.4275, -0.4275, -0.4275],\n",
      "          [ 0.5216,  0.5059,  0.4902,  ..., -0.4353, -0.4353, -0.4353]],\n",
      "\n",
      "         [[ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          [ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          [ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5373,  0.5216,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [ 0.5451,  0.5373,  0.5216,  ..., -0.1922, -0.1922, -0.1922],\n",
      "          [ 0.5451,  0.5294,  0.5137,  ..., -0.2000, -0.2000, -0.2000]]],\n",
      "\n",
      "\n",
      "        [[[-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          [-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          [-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          ...,\n",
      "          [ 0.5922,  0.5843,  0.5686,  ..., -0.5608, -0.5608, -0.5608],\n",
      "          [ 0.5922,  0.5843,  0.5686,  ..., -0.5765, -0.5765, -0.5765],\n",
      "          [ 0.5922,  0.5765,  0.5608,  ..., -0.5843, -0.5843, -0.5843]],\n",
      "\n",
      "         [[-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          ...,\n",
      "          [ 0.5216,  0.5137,  0.4980,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [ 0.5216,  0.5137,  0.4980,  ..., -0.4275, -0.4275, -0.4275],\n",
      "          [ 0.5216,  0.5059,  0.4902,  ..., -0.4353, -0.4353, -0.4353]],\n",
      "\n",
      "         [[ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          [ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          [ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5373,  0.5216,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [ 0.5451,  0.5373,  0.5216,  ..., -0.1922, -0.1922, -0.1922],\n",
      "          [ 0.5451,  0.5294,  0.5137,  ..., -0.2000, -0.2000, -0.2000]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          [-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          [-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          ...,\n",
      "          [ 0.5922,  0.5843,  0.5686,  ..., -0.5608, -0.5608, -0.5608],\n",
      "          [ 0.5922,  0.5843,  0.5686,  ..., -0.5765, -0.5765, -0.5765],\n",
      "          [ 0.5922,  0.5765,  0.5608,  ..., -0.5843, -0.5843, -0.5843]],\n",
      "\n",
      "         [[-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          ...,\n",
      "          [ 0.5216,  0.5137,  0.4980,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [ 0.5216,  0.5137,  0.4980,  ..., -0.4275, -0.4275, -0.4275],\n",
      "          [ 0.5216,  0.5059,  0.4902,  ..., -0.4353, -0.4353, -0.4353]],\n",
      "\n",
      "         [[ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          [ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          [ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5373,  0.5216,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [ 0.5451,  0.5373,  0.5216,  ..., -0.1922, -0.1922, -0.1922],\n",
      "          [ 0.5451,  0.5294,  0.5137,  ..., -0.2000, -0.2000, -0.2000]]],\n",
      "\n",
      "\n",
      "        [[[-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          [-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          [-0.5922, -0.5922, -0.5922,  ..., -0.6392, -0.6392, -0.6392],\n",
      "          ...,\n",
      "          [ 0.5922,  0.5843,  0.5686,  ..., -0.5608, -0.5608, -0.5608],\n",
      "          [ 0.5922,  0.5843,  0.5686,  ..., -0.5765, -0.5765, -0.5765],\n",
      "          [ 0.5922,  0.5765,  0.5608,  ..., -0.5843, -0.5843, -0.5843]],\n",
      "\n",
      "         [[-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          [-0.2314, -0.2314, -0.2314,  ..., -0.2549, -0.2549, -0.2549],\n",
      "          ...,\n",
      "          [ 0.5216,  0.5137,  0.4980,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [ 0.5216,  0.5137,  0.4980,  ..., -0.4275, -0.4275, -0.4275],\n",
      "          [ 0.5216,  0.5059,  0.4902,  ..., -0.4353, -0.4353, -0.4353]],\n",
      "\n",
      "         [[ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          [ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          [ 0.2314,  0.2314,  0.2314,  ...,  0.2627,  0.2627,  0.2627],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5373,  0.5216,  ..., -0.1765, -0.1765, -0.1765],\n",
      "          [ 0.5451,  0.5373,  0.5216,  ..., -0.1922, -0.1922, -0.1922],\n",
      "          [ 0.5451,  0.5294,  0.5137,  ..., -0.2000, -0.2000, -0.2000]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[0.6549, 0.6549, 0.6549,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.6549, 0.6549, 0.6549,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.6471, 0.6471, 0.6471,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          ...,\n",
      "          [0.4118, 0.4118, 0.4039,  ..., 0.6000, 0.5608, 0.5451],\n",
      "          [0.4275, 0.4275, 0.4118,  ..., 0.6000, 0.5529, 0.5294],\n",
      "          [0.4353, 0.4353, 0.4196,  ..., 0.6000, 0.5451, 0.5216]],\n",
      "\n",
      "         [[0.7961, 0.7961, 0.7961,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          [0.7961, 0.7961, 0.7961,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          [0.7882, 0.7882, 0.7882,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          ...,\n",
      "          [0.4039, 0.4039, 0.3961,  ..., 0.7255, 0.7020, 0.6784],\n",
      "          [0.4196, 0.4196, 0.4039,  ..., 0.7333, 0.6941, 0.6706],\n",
      "          [0.4275, 0.4275, 0.4118,  ..., 0.7333, 0.6863, 0.6627]],\n",
      "\n",
      "         [[0.9686, 0.9686, 0.9686,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          [0.9686, 0.9686, 0.9608,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          ...,\n",
      "          [0.3647, 0.3647, 0.3569,  ..., 0.7333, 0.7098, 0.6941],\n",
      "          [0.3804, 0.3804, 0.3647,  ..., 0.7490, 0.7098, 0.6941],\n",
      "          [0.3882, 0.3882, 0.3725,  ..., 0.7569, 0.7098, 0.6941]]],\n",
      "\n",
      "\n",
      "        [[[0.6549, 0.6549, 0.6549,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.6549, 0.6549, 0.6549,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.6471, 0.6471, 0.6471,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          ...,\n",
      "          [0.4118, 0.4118, 0.4039,  ..., 0.6000, 0.5608, 0.5451],\n",
      "          [0.4275, 0.4275, 0.4118,  ..., 0.6000, 0.5529, 0.5294],\n",
      "          [0.4353, 0.4353, 0.4196,  ..., 0.6000, 0.5451, 0.5216]],\n",
      "\n",
      "         [[0.7961, 0.7961, 0.7961,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          [0.7961, 0.7961, 0.7961,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          [0.7882, 0.7882, 0.7882,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          ...,\n",
      "          [0.4039, 0.4039, 0.3961,  ..., 0.7255, 0.7020, 0.6784],\n",
      "          [0.4196, 0.4196, 0.4039,  ..., 0.7333, 0.6941, 0.6706],\n",
      "          [0.4275, 0.4275, 0.4118,  ..., 0.7333, 0.6863, 0.6627]],\n",
      "\n",
      "         [[0.9686, 0.9686, 0.9686,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          [0.9686, 0.9686, 0.9608,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          ...,\n",
      "          [0.3647, 0.3647, 0.3569,  ..., 0.7333, 0.7098, 0.6941],\n",
      "          [0.3804, 0.3804, 0.3647,  ..., 0.7490, 0.7098, 0.6941],\n",
      "          [0.3882, 0.3882, 0.3725,  ..., 0.7569, 0.7098, 0.6941]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[0.6549, 0.6549, 0.6549,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.6549, 0.6549, 0.6549,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.6471, 0.6471, 0.6471,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          ...,\n",
      "          [0.4118, 0.4118, 0.4039,  ..., 0.6000, 0.5608, 0.5451],\n",
      "          [0.4275, 0.4275, 0.4118,  ..., 0.6000, 0.5529, 0.5294],\n",
      "          [0.4353, 0.4353, 0.4196,  ..., 0.6000, 0.5451, 0.5216]],\n",
      "\n",
      "         [[0.7961, 0.7961, 0.7961,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          [0.7961, 0.7961, 0.7961,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          [0.7882, 0.7882, 0.7882,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          ...,\n",
      "          [0.4039, 0.4039, 0.3961,  ..., 0.7255, 0.7020, 0.6784],\n",
      "          [0.4196, 0.4196, 0.4039,  ..., 0.7333, 0.6941, 0.6706],\n",
      "          [0.4275, 0.4275, 0.4118,  ..., 0.7333, 0.6863, 0.6627]],\n",
      "\n",
      "         [[0.9686, 0.9686, 0.9686,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          [0.9686, 0.9686, 0.9608,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          ...,\n",
      "          [0.3647, 0.3647, 0.3569,  ..., 0.7333, 0.7098, 0.6941],\n",
      "          [0.3804, 0.3804, 0.3647,  ..., 0.7490, 0.7098, 0.6941],\n",
      "          [0.3882, 0.3882, 0.3725,  ..., 0.7569, 0.7098, 0.6941]]],\n",
      "\n",
      "\n",
      "        [[[0.6549, 0.6549, 0.6549,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.6549, 0.6549, 0.6549,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.6471, 0.6471, 0.6471,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          ...,\n",
      "          [0.4118, 0.4118, 0.4039,  ..., 0.6000, 0.5608, 0.5451],\n",
      "          [0.4275, 0.4275, 0.4118,  ..., 0.6000, 0.5529, 0.5294],\n",
      "          [0.4353, 0.4353, 0.4196,  ..., 0.6000, 0.5451, 0.5216]],\n",
      "\n",
      "         [[0.7961, 0.7961, 0.7961,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          [0.7961, 0.7961, 0.7961,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          [0.7882, 0.7882, 0.7882,  ..., 0.6863, 0.6863, 0.6863],\n",
      "          ...,\n",
      "          [0.4039, 0.4039, 0.3961,  ..., 0.7255, 0.7020, 0.6784],\n",
      "          [0.4196, 0.4196, 0.4039,  ..., 0.7333, 0.6941, 0.6706],\n",
      "          [0.4275, 0.4275, 0.4118,  ..., 0.7333, 0.6863, 0.6627]],\n",
      "\n",
      "         [[0.9686, 0.9686, 0.9686,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          [0.9686, 0.9686, 0.9686,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          [0.9686, 0.9686, 0.9608,  ..., 0.8745, 0.8745, 0.8745],\n",
      "          ...,\n",
      "          [0.3647, 0.3647, 0.3569,  ..., 0.7333, 0.7098, 0.6941],\n",
      "          [0.3804, 0.3804, 0.3647,  ..., 0.7490, 0.7098, 0.6941],\n",
      "          [0.3882, 0.3882, 0.3725,  ..., 0.7569, 0.7098, 0.6941]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          [-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          [-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          ...,\n",
      "          [-0.1451, -0.1451, -0.1608,  ..., -0.3725, -0.3490, -0.3333],\n",
      "          [-0.0902, -0.0980, -0.1373,  ..., -0.3725, -0.3412, -0.3255],\n",
      "          [-0.0667, -0.0824, -0.1216,  ..., -0.3725, -0.3412, -0.3176]],\n",
      "\n",
      "         [[-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          [-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          [-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          ...,\n",
      "          [-0.4275, -0.4275, -0.4431,  ..., -0.5529, -0.5294, -0.5137],\n",
      "          [-0.3725, -0.3804, -0.4196,  ..., -0.5529, -0.5216, -0.5059],\n",
      "          [-0.3490, -0.3647, -0.4039,  ..., -0.5529, -0.5216, -0.4980]],\n",
      "\n",
      "         [[ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.7020,  ..., -0.8196, -0.7961, -0.7882],\n",
      "          [-0.6627, -0.6706, -0.7020,  ..., -0.8196, -0.7882, -0.7725],\n",
      "          [-0.6471, -0.6627, -0.7020,  ..., -0.8196, -0.7804, -0.7647]]],\n",
      "\n",
      "\n",
      "        [[[-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          [-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          [-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          ...,\n",
      "          [-0.1451, -0.1451, -0.1608,  ..., -0.3725, -0.3490, -0.3333],\n",
      "          [-0.0902, -0.0980, -0.1373,  ..., -0.3725, -0.3412, -0.3255],\n",
      "          [-0.0667, -0.0824, -0.1216,  ..., -0.3725, -0.3412, -0.3176]],\n",
      "\n",
      "         [[-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          [-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          [-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          ...,\n",
      "          [-0.4275, -0.4275, -0.4431,  ..., -0.5529, -0.5294, -0.5137],\n",
      "          [-0.3725, -0.3804, -0.4196,  ..., -0.5529, -0.5216, -0.5059],\n",
      "          [-0.3490, -0.3647, -0.4039,  ..., -0.5529, -0.5216, -0.4980]],\n",
      "\n",
      "         [[ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.7020,  ..., -0.8196, -0.7961, -0.7882],\n",
      "          [-0.6627, -0.6706, -0.7020,  ..., -0.8196, -0.7882, -0.7725],\n",
      "          [-0.6471, -0.6627, -0.7020,  ..., -0.8196, -0.7804, -0.7647]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          [-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          [-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          ...,\n",
      "          [-0.1451, -0.1451, -0.1608,  ..., -0.3725, -0.3490, -0.3333],\n",
      "          [-0.0902, -0.0980, -0.1373,  ..., -0.3725, -0.3412, -0.3255],\n",
      "          [-0.0667, -0.0824, -0.1216,  ..., -0.3725, -0.3412, -0.3176]],\n",
      "\n",
      "         [[-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          [-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          [-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          ...,\n",
      "          [-0.4275, -0.4275, -0.4431,  ..., -0.5529, -0.5294, -0.5137],\n",
      "          [-0.3725, -0.3804, -0.4196,  ..., -0.5529, -0.5216, -0.5059],\n",
      "          [-0.3490, -0.3647, -0.4039,  ..., -0.5529, -0.5216, -0.4980]],\n",
      "\n",
      "         [[ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.7020,  ..., -0.8196, -0.7961, -0.7882],\n",
      "          [-0.6627, -0.6706, -0.7020,  ..., -0.8196, -0.7882, -0.7725],\n",
      "          [-0.6471, -0.6627, -0.7020,  ..., -0.8196, -0.7804, -0.7647]]],\n",
      "\n",
      "\n",
      "        [[[-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          [-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          [-0.6392, -0.6392, -0.6392,  ..., -0.6314, -0.6392, -0.6392],\n",
      "          ...,\n",
      "          [-0.1451, -0.1451, -0.1608,  ..., -0.3725, -0.3490, -0.3333],\n",
      "          [-0.0902, -0.0980, -0.1373,  ..., -0.3725, -0.3412, -0.3255],\n",
      "          [-0.0667, -0.0824, -0.1216,  ..., -0.3725, -0.3412, -0.3176]],\n",
      "\n",
      "         [[-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          [-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          [-0.4353, -0.4353, -0.4353,  ..., -0.3882, -0.3961, -0.3961],\n",
      "          ...,\n",
      "          [-0.4275, -0.4275, -0.4431,  ..., -0.5529, -0.5294, -0.5137],\n",
      "          [-0.3725, -0.3804, -0.4196,  ..., -0.5529, -0.5216, -0.5059],\n",
      "          [-0.3490, -0.3647, -0.4039,  ..., -0.5529, -0.5216, -0.4980]],\n",
      "\n",
      "         [[ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          [ 0.0118,  0.0118,  0.0118,  ...,  0.0353,  0.0275,  0.0275],\n",
      "          ...,\n",
      "          [-0.6863, -0.6863, -0.7020,  ..., -0.8196, -0.7961, -0.7882],\n",
      "          [-0.6627, -0.6706, -0.7020,  ..., -0.8196, -0.7882, -0.7725],\n",
      "          [-0.6471, -0.6627, -0.7020,  ..., -0.8196, -0.7804, -0.7647]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.1059,  0.0980,  0.0667,  ...,  0.5059,  0.5059,  0.5059],\n",
      "          [ 0.1059,  0.0980,  0.0588,  ...,  0.5059,  0.5059,  0.5059],\n",
      "          [ 0.0902,  0.0824,  0.0510,  ...,  0.4980,  0.4980,  0.4980],\n",
      "          ...,\n",
      "          [ 0.8824,  0.8667,  0.8353,  ..., -0.5608, -0.5686, -0.5686],\n",
      "          [ 0.9686,  0.9608,  0.9137,  ..., -0.5686, -0.5765, -0.5765],\n",
      "          [ 1.0000,  1.0000,  0.9451,  ..., -0.5686, -0.5765, -0.5765]],\n",
      "\n",
      "         [[-0.2706, -0.2784, -0.3098,  ...,  0.1373,  0.1373,  0.1373],\n",
      "          [-0.2706, -0.2784, -0.3176,  ...,  0.1373,  0.1373,  0.1373],\n",
      "          [-0.2863, -0.2941, -0.3255,  ...,  0.1294,  0.1294,  0.1294],\n",
      "          ...,\n",
      "          [ 0.8588,  0.8431,  0.8118,  ..., -0.7647, -0.7647, -0.7647],\n",
      "          [ 0.9686,  0.9529,  0.8980,  ..., -0.7804, -0.7804, -0.7804],\n",
      "          [ 1.0000,  1.0000,  0.9451,  ..., -0.7882, -0.7882, -0.7882]],\n",
      "\n",
      "         [[-0.5529, -0.5608, -0.5922,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [-0.5529, -0.5608, -0.6000,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [-0.5686, -0.5765, -0.6078,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          ...,\n",
      "          [ 0.8275,  0.8039,  0.7725,  ..., -0.8196, -0.8196, -0.8196],\n",
      "          [ 0.9373,  0.9137,  0.8667,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [ 0.9922,  0.9686,  0.9137,  ..., -0.8431, -0.8431, -0.8431]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1059,  0.0980,  0.0667,  ...,  0.5059,  0.5059,  0.5059],\n",
      "          [ 0.1059,  0.0980,  0.0588,  ...,  0.5059,  0.5059,  0.5059],\n",
      "          [ 0.0902,  0.0824,  0.0510,  ...,  0.4980,  0.4980,  0.4980],\n",
      "          ...,\n",
      "          [ 0.8824,  0.8667,  0.8353,  ..., -0.5608, -0.5686, -0.5686],\n",
      "          [ 0.9686,  0.9608,  0.9137,  ..., -0.5686, -0.5765, -0.5765],\n",
      "          [ 1.0000,  1.0000,  0.9451,  ..., -0.5686, -0.5765, -0.5765]],\n",
      "\n",
      "         [[-0.2706, -0.2784, -0.3098,  ...,  0.1373,  0.1373,  0.1373],\n",
      "          [-0.2706, -0.2784, -0.3176,  ...,  0.1373,  0.1373,  0.1373],\n",
      "          [-0.2863, -0.2941, -0.3255,  ...,  0.1294,  0.1294,  0.1294],\n",
      "          ...,\n",
      "          [ 0.8588,  0.8431,  0.8118,  ..., -0.7647, -0.7647, -0.7647],\n",
      "          [ 0.9686,  0.9529,  0.8980,  ..., -0.7804, -0.7804, -0.7804],\n",
      "          [ 1.0000,  1.0000,  0.9451,  ..., -0.7882, -0.7882, -0.7882]],\n",
      "\n",
      "         [[-0.5529, -0.5608, -0.5922,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [-0.5529, -0.5608, -0.6000,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [-0.5686, -0.5765, -0.6078,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          ...,\n",
      "          [ 0.8275,  0.8039,  0.7725,  ..., -0.8196, -0.8196, -0.8196],\n",
      "          [ 0.9373,  0.9137,  0.8667,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [ 0.9922,  0.9686,  0.9137,  ..., -0.8431, -0.8431, -0.8431]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n",
      "=========== input check ==============\n",
      "None\n",
      "None\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n",
      "None\n",
      "None\n",
      "tensor([[[[ 0.1059,  0.0980,  0.0667,  ...,  0.5059,  0.5059,  0.5059],\n",
      "          [ 0.1059,  0.0980,  0.0588,  ...,  0.5059,  0.5059,  0.5059],\n",
      "          [ 0.0902,  0.0824,  0.0510,  ...,  0.4980,  0.4980,  0.4980],\n",
      "          ...,\n",
      "          [ 0.8824,  0.8667,  0.8353,  ..., -0.5608, -0.5686, -0.5686],\n",
      "          [ 0.9686,  0.9608,  0.9137,  ..., -0.5686, -0.5765, -0.5765],\n",
      "          [ 1.0000,  1.0000,  0.9451,  ..., -0.5686, -0.5765, -0.5765]],\n",
      "\n",
      "         [[-0.2706, -0.2784, -0.3098,  ...,  0.1373,  0.1373,  0.1373],\n",
      "          [-0.2706, -0.2784, -0.3176,  ...,  0.1373,  0.1373,  0.1373],\n",
      "          [-0.2863, -0.2941, -0.3255,  ...,  0.1294,  0.1294,  0.1294],\n",
      "          ...,\n",
      "          [ 0.8588,  0.8431,  0.8118,  ..., -0.7647, -0.7647, -0.7647],\n",
      "          [ 0.9686,  0.9529,  0.8980,  ..., -0.7804, -0.7804, -0.7804],\n",
      "          [ 1.0000,  1.0000,  0.9451,  ..., -0.7882, -0.7882, -0.7882]],\n",
      "\n",
      "         [[-0.5529, -0.5608, -0.5922,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [-0.5529, -0.5608, -0.6000,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [-0.5686, -0.5765, -0.6078,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          ...,\n",
      "          [ 0.8275,  0.8039,  0.7725,  ..., -0.8196, -0.8196, -0.8196],\n",
      "          [ 0.9373,  0.9137,  0.8667,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [ 0.9922,  0.9686,  0.9137,  ..., -0.8431, -0.8431, -0.8431]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1059,  0.0980,  0.0667,  ...,  0.5059,  0.5059,  0.5059],\n",
      "          [ 0.1059,  0.0980,  0.0588,  ...,  0.5059,  0.5059,  0.5059],\n",
      "          [ 0.0902,  0.0824,  0.0510,  ...,  0.4980,  0.4980,  0.4980],\n",
      "          ...,\n",
      "          [ 0.8824,  0.8667,  0.8353,  ..., -0.5608, -0.5686, -0.5686],\n",
      "          [ 0.9686,  0.9608,  0.9137,  ..., -0.5686, -0.5765, -0.5765],\n",
      "          [ 1.0000,  1.0000,  0.9451,  ..., -0.5686, -0.5765, -0.5765]],\n",
      "\n",
      "         [[-0.2706, -0.2784, -0.3098,  ...,  0.1373,  0.1373,  0.1373],\n",
      "          [-0.2706, -0.2784, -0.3176,  ...,  0.1373,  0.1373,  0.1373],\n",
      "          [-0.2863, -0.2941, -0.3255,  ...,  0.1294,  0.1294,  0.1294],\n",
      "          ...,\n",
      "          [ 0.8588,  0.8431,  0.8118,  ..., -0.7647, -0.7647, -0.7647],\n",
      "          [ 0.9686,  0.9529,  0.8980,  ..., -0.7804, -0.7804, -0.7804],\n",
      "          [ 1.0000,  1.0000,  0.9451,  ..., -0.7882, -0.7882, -0.7882]],\n",
      "\n",
      "         [[-0.5529, -0.5608, -0.5922,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [-0.5529, -0.5608, -0.6000,  ..., -0.4118, -0.4118, -0.4118],\n",
      "          [-0.5686, -0.5765, -0.6078,  ..., -0.4196, -0.4196, -0.4196],\n",
      "          ...,\n",
      "          [ 0.8275,  0.8039,  0.7725,  ..., -0.8196, -0.8196, -0.8196],\n",
      "          [ 0.9373,  0.9137,  0.8667,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [ 0.9922,  0.9686,  0.9137,  ..., -0.8431, -0.8431, -0.8431]]]],\n",
      "       device='cuda:0')\n",
      "=========== input check ==============\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55c179-842b-4fa8-8aea-a6b192b3933c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
